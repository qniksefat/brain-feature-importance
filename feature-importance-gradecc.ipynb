{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # for plotting facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# todo aggregating regions together for easier model prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = 'feature_metric.csv'\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "df = df[['region', 'measure', 'epoch_', 'value',\n",
    "       'Subject', 'Score_median_first20']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### agg cortex into 14 nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "0       7Networks_LH_Vis_1\n1       7Networks_LH_Vis_2\n2       7Networks_LH_Vis_3\n3       7Networks_LH_Vis_4\n4       7Networks_LH_Vis_5\n               ...        \n1035             Vermis IX\n1036              Right IX\n1037                Left X\n1038              Vermis X\n1039               Right X\nName: region, Length: 1040, dtype: object"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = df.region.drop_duplicates()\n",
    "r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "    hemisphere  network position num\n0           LH      Vis     None   1\n1           LH      Vis     None   2\n2           LH      Vis     None   3\n3           LH      Vis     None   4\n4           LH      Vis     None   5\n..         ...      ...      ...  ..\n993         RH  Default  pCunPCC  16\n994         RH  Default  pCunPCC  17\n995         RH  Default  pCunPCC  18\n996         RH     Cont     pCun   2\n997         RH     Cont     pCun   4\n\n[998 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hemisphere</th>\n      <th>network</th>\n      <th>position</th>\n      <th>num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LH</td>\n      <td>Vis</td>\n      <td>None</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LH</td>\n      <td>Vis</td>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LH</td>\n      <td>Vis</td>\n      <td>None</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LH</td>\n      <td>Vis</td>\n      <td>None</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LH</td>\n      <td>Vis</td>\n      <td>None</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>RH</td>\n      <td>Default</td>\n      <td>pCunPCC</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>RH</td>\n      <td>Default</td>\n      <td>pCunPCC</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>RH</td>\n      <td>Default</td>\n      <td>pCunPCC</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>RH</td>\n      <td>Cont</td>\n      <td>pCun</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>RH</td>\n      <td>Cont</td>\n      <td>pCun</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>998 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortex = r[:998].apply(lambda x: x[10:].split('_'))\n",
    "cortex = cortex.tolist()\n",
    "cortex = pd.DataFrame(cortex, columns=['hemisphere', 'network', 'position', 'num'])\n",
    "cortex['num'] = cortex.apply(lambda r: r['position'] if not r['num'] else r['num'], axis=1)\n",
    "cortex['position'] = cortex.apply(lambda r: None if not r['num'] != r['position'] else r['position'], axis=1)\n",
    "cortex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "       hemisphere  network position  num\ncount         998      998      643  998\nunique          2        7       23  103\ntop            LH  Default     Post    1\nfreq          500      212      102   49",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hemisphere</th>\n      <th>network</th>\n      <th>position</th>\n      <th>num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>998</td>\n      <td>998</td>\n      <td>643</td>\n      <td>998</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n      <td>7</td>\n      <td>23</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>LH</td>\n      <td>Default</td>\n      <td>Post</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>500</td>\n      <td>212</td>\n      <td>102</td>\n      <td>49</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortex.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "       hemisphere network position  num\ncount          42      42        6    0\nunique          3      16        2    0\ntop          Left    Crus        I  NaN\nfreq           17       6        3  NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hemisphere</th>\n      <th>network</th>\n      <th>position</th>\n      <th>num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>42</td>\n      <td>42</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>3</td>\n      <td>16</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Left</td>\n      <td>Crus</td>\n      <td>I</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>17</td>\n      <td>6</td>\n      <td>3</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subc = r[998:].apply(lambda x: x.split(' '))\n",
    "subc = subc.to_list()\n",
    "subc = pd.DataFrame(subc, columns=['hemisphere', 'network', 'position'])\n",
    "subc['num'] = None\n",
    "subc.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "cortex['r'] = 'cor_' + cortex['hemisphere'] + '_' + cortex['network']\n",
    "subc['r'] = 'subc_' + subc['hemisphere']\n",
    "# 14 regions from cortex, 3 regions from subc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "0       7Networks_LH_Vis_1\n1       7Networks_LH_Vis_2\n2       7Networks_LH_Vis_3\n3       7Networks_LH_Vis_4\n4       7Networks_LH_Vis_5\n               ...        \n1035             Vermis IX\n1036              Right IX\n1037                Left X\n1038              Vermis X\n1039               Right X\nName: region, Length: 1040, dtype: object"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "       region_agg              region\n0      cor_LH_Vis  7Networks_LH_Vis_1\n1      cor_LH_Vis  7Networks_LH_Vis_2\n2      cor_LH_Vis  7Networks_LH_Vis_3\n3      cor_LH_Vis  7Networks_LH_Vis_4\n4      cor_LH_Vis  7Networks_LH_Vis_5\n...           ...                 ...\n1035  subc_Vermis           Vermis IX\n1036   subc_Right            Right IX\n1037    subc_Left              Left X\n1038  subc_Vermis            Vermis X\n1039   subc_Right             Right X\n\n[1040 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_agg</th>\n      <th>region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cor_LH_Vis</td>\n      <td>7Networks_LH_Vis_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cor_LH_Vis</td>\n      <td>7Networks_LH_Vis_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cor_LH_Vis</td>\n      <td>7Networks_LH_Vis_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cor_LH_Vis</td>\n      <td>7Networks_LH_Vis_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cor_LH_Vis</td>\n      <td>7Networks_LH_Vis_5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1035</th>\n      <td>subc_Vermis</td>\n      <td>Vermis IX</td>\n    </tr>\n    <tr>\n      <th>1036</th>\n      <td>subc_Right</td>\n      <td>Right IX</td>\n    </tr>\n    <tr>\n      <th>1037</th>\n      <td>subc_Left</td>\n      <td>Left X</td>\n    </tr>\n    <tr>\n      <th>1038</th>\n      <td>subc_Vermis</td>\n      <td>Vermis X</td>\n    </tr>\n    <tr>\n      <th>1039</th>\n      <td>subc_Right</td>\n      <td>Right X</td>\n    </tr>\n  </tbody>\n</table>\n<p>1040 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_agg = pd.concat([cortex.r, subc.r], axis=0).reset_index(drop=True)\n",
    "regions_agg = pd.concat([regions_agg.rename('region_agg'), r], axis=1)\n",
    "regions_agg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "dfecc = df[df['measure'] == 'eccentricity']\n",
    "dfecc = dfecc.merge(regions_agg, on='region')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "Subject  measure       epoch_  region_agg        \nAB1      eccentricity  b2e     cor_LH_Cont           0.096832\n                               cor_LH_Default        0.285795\n                               cor_LH_DorsAttn       0.336393\n                               cor_LH_Limbic         0.191118\n                               cor_LH_SalVentAttn    0.141011\n                                                       ...   \nZN1      eccentricity  late    cor_RH_SomMot         2.245211\n                               cor_RH_Vis            2.400097\n                               subc_Left             0.873998\n                               subc_Right            0.920408\n                               subc_Vermis           0.869525\nName: value, Length: 3060, dtype: float64"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfecc.groupby(['Subject', 'measure', 'epoch_', 'region_agg'])['value'].mean()\n",
    "# by default, pivot_table takes mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "                                   value                                 \\\nepoch_                               b2e                                  \nregion_agg                   cor_LH_Cont cor_LH_Default cor_LH_DorsAttn   \nSubject Score_median_first20                                              \nAB1     14.0                    0.096832       0.285795        0.336393   \nAG1     47.0                    0.023247       0.134270        0.016678   \nAH1     41.5                   -0.323308      -0.037860       -0.711544   \nAM1     27.0                    0.175577       0.412751        0.288554   \nAP1     17.0                    0.467508       0.169592       -0.193752   \nAV1     48.5                   -0.038907      -0.278500        0.676340   \nBN1     75.0                    0.056349      -0.016979       -0.353304   \nCD1     15.0                    0.051840      -0.055279       -0.196941   \nCG1     32.0                    0.108409       0.024720        0.175183   \nCH1     39.0                    0.177342      -0.087421       -0.016997   \nCK1     34.0                    0.086924       0.137110        0.299040   \nCM1     29.5                    0.210084      -0.287603        0.282841   \nCM2     73.0                    0.268694       0.294601       -0.253968   \nEH1     21.0                    0.387339       0.257275        0.291328   \nEK1     12.0                    0.013718      -0.145739        0.582988   \nHK1     17.0                    0.321216       0.582425        0.415549   \nHS1     67.0                    0.170226      -0.168491        0.353160   \nJC1     12.5                    0.441986       0.234091        0.437221   \nJH1     8.5                    -0.026666       0.106128        0.104092   \nJM1     34.0                   -0.281974      -0.076701        0.006779   \nJR1     34.0                   -0.072512       0.251876       -0.125334   \nJS1     57.0                    0.057821       0.119935        0.076817   \nKK1     11.5                   -0.025902       0.484708        0.069643   \nKP1     38.0                    0.131821       0.172627        0.242722   \nKR1     25.0                    0.492554       0.689035        0.391546   \nLB1     12.0                   -0.088604       0.422639        0.162973   \nMB1     39.0                   -0.056847       0.236683       -0.310333   \nMG1     57.0                   -0.252071      -0.571489       -0.340796   \nMP1     45.0                    0.038101      -0.436873       -0.311200   \nNW1     26.0                   -0.284932      -0.155696        0.231738   \nSK1     39.0                    0.353440       0.344001       -0.212267   \nSM2     44.0                    0.156058       0.257919        0.384763   \nSR1     21.5                   -0.047300      -0.070251       -0.042661   \nUP1     46.0                    0.141589       0.151221        0.142272   \nWK1     62.0                    0.068048       0.233606        0.053965   \nZN1     55.0                    0.146824      -0.541973       -0.176711   \n\n                                                                             \\\nepoch_                                                                        \nregion_agg                   cor_LH_Limbic cor_LH_SalVentAttn cor_LH_SomMot   \nSubject Score_median_first20                                                  \nAB1     14.0                      0.191118           0.141011      0.004024   \nAG1     47.0                     -0.172098           0.584519      0.466026   \nAH1     41.5                     -0.305593          -0.227577      0.013477   \nAM1     27.0                      0.048740           0.684970      0.351564   \nAP1     17.0                     -0.240978           0.309113     -0.226975   \nAV1     48.5                     -0.182464           0.137662      0.267408   \nBN1     75.0                     -0.461987          -0.095968     -0.302246   \nCD1     15.0                     -0.095444          -0.239501     -0.159295   \nCG1     32.0                     -0.050737          -0.377077     -0.248930   \nCH1     39.0                     -0.433329          -0.306632     -0.416012   \nCK1     34.0                     -0.123841           0.162907      0.514591   \nCM1     29.5                      0.191320          -0.181049      0.408398   \nCM2     73.0                      0.040571          -0.332977     -0.143175   \nEH1     21.0                      0.031696           0.448034      0.225895   \nEK1     12.0                      0.007816           0.292638      0.670939   \nHK1     17.0                      0.005094           0.273420      0.180396   \nHS1     67.0                     -0.060915           0.104842      0.046738   \nJC1     12.5                     -0.171470           0.216009     -0.057136   \nJH1     8.5                       0.458677          -0.207485     -0.334236   \nJM1     34.0                     -0.074407           0.129751      0.341420   \nJR1     34.0                      0.136915          -0.049604     -0.354429   \nJS1     57.0                     -0.161564           0.178161      0.385102   \nKK1     11.5                     -0.103952           0.453943      0.125557   \nKP1     38.0                     -0.000527           0.143837     -0.173275   \nKR1     25.0                      0.061062           0.117760      0.363120   \nLB1     12.0                     -0.061952           0.070794     -0.100209   \nMB1     39.0                     -0.420323           0.395842     -0.279680   \nMG1     57.0                     -0.205559          -0.285394     -0.190710   \nMP1     45.0                     -0.566607          -0.674823     -0.209500   \nNW1     26.0                     -0.321735           0.158029      0.738793   \nSK1     39.0                      0.011286          -0.277125     -0.489517   \nSM2     44.0                      0.080896           0.152030      0.202089   \nSR1     21.5                     -0.290926          -0.490961      0.141022   \nUP1     46.0                      0.022815           0.133355      0.163306   \nWK1     62.0                      0.047418          -0.042987      0.142878   \nZN1     55.0                     -0.314063           0.160933     -0.103045   \n\n                                                                    \\\nepoch_                                                               \nregion_agg                   cor_LH_Vis cor_RH_Cont cor_RH_Default   \nSubject Score_median_first20                                         \nAB1     14.0                   0.448154   -0.133301      -0.009500   \nAG1     47.0                  -0.106237    0.255863      -0.195050   \nAH1     41.5                  -0.423758   -0.185166      -0.366852   \nAM1     27.0                   0.214291    0.090384       0.558743   \nAP1     17.0                   0.522462   -0.106493      -0.040279   \nAV1     48.5                  -0.640726   -0.031024      -0.274715   \nBN1     75.0                  -0.727874   -0.240385      -0.361364   \nCD1     15.0                   0.439867    0.079581      -0.485805   \nCG1     32.0                  -0.497208    0.301012      -0.195419   \nCH1     39.0                  -0.262217    0.006448      -0.122991   \nCK1     34.0                   0.492125    0.150974       0.323942   \nCM1     29.5                  -0.415814    0.109322      -0.269060   \nCM2     73.0                  -0.212977    0.415070       0.127135   \nEH1     21.0                  -0.088108    0.405360       0.145292   \nEK1     12.0                   0.623340   -0.264783       0.040501   \nHK1     17.0                   0.045845    0.132566       0.025067   \nHS1     67.0                   0.540557    0.363904      -0.218475   \nJC1     12.5                   0.280812   -0.175299       0.204475   \nJH1     8.5                    0.130646   -0.122474       0.023361   \nJM1     34.0                  -0.310428   -0.134713      -0.202092   \nJR1     34.0                  -0.095955    0.192945      -0.159106   \nJS1     57.0                  -0.150975   -0.237924      -0.045934   \nKK1     11.5                   0.744675    0.217756       0.099546   \nKP1     38.0                   0.350981    0.013444      -0.083172   \nKR1     25.0                   0.007332    0.464407       0.486510   \nLB1     12.0                   0.464483    0.166278      -0.112810   \nMB1     39.0                  -0.298043   -0.335835      -0.135304   \nMG1     57.0                  -0.268854   -0.198966      -0.395889   \nMP1     45.0                  -1.388130   -0.376981      -0.475804   \nNW1     26.0                  -0.509783   -0.087884      -0.138356   \nSK1     39.0                  -0.061978    0.330996       0.195117   \nSM2     44.0                   0.151459   -0.094142      -0.164098   \nSR1     21.5                  -0.125938   -0.084477      -0.511392   \nUP1     46.0                   1.118926   -0.426311      -0.036800   \nWK1     62.0                  -0.112929    0.186223       0.062975   \nZN1     55.0                  -0.715399   -0.297575      -0.164273   \n\n                                              ...                             \\\nepoch_                                        ...        late                  \nregion_agg                   cor_RH_DorsAttn  ... cor_RH_Cont cor_RH_Default   \nSubject Score_median_first20                  ...                              \nAB1     14.0                        0.718707  ...    1.718728       1.976566   \nAG1     47.0                        0.175015  ...    2.118806       1.950530   \nAH1     41.5                       -0.494792  ...    1.469235       1.521187   \nAM1     27.0                       -0.080070  ...    1.986381       2.447330   \nAP1     17.0                       -0.234033  ...    1.900803       1.932110   \nAV1     48.5                        0.355416  ...    1.825501       1.970395   \nBN1     75.0                       -0.243420  ...    1.828686       2.417146   \nCD1     15.0                       -0.051421  ...    1.917480       1.779552   \nCG1     32.0                        0.396552  ...    1.843634       1.816909   \nCH1     39.0                       -0.126323  ...    1.691757       1.820045   \nCK1     34.0                        0.202864  ...    1.802336       1.631477   \nCM1     29.5                       -0.058011  ...    1.714557       1.964727   \nCM2     73.0                        0.055575  ...    2.143757       1.788549   \nEH1     21.0                        0.361376  ...    2.139039       2.438279   \nEK1     12.0                        0.387089  ...    1.998436       1.958971   \nHK1     17.0                        0.036989  ...    2.017204       1.528434   \nHS1     67.0                        0.434677  ...    1.749179       1.640362   \nJC1     12.5                        0.016970  ...    2.017339       1.882053   \nJH1     8.5                         0.134300  ...    1.998840       2.415192   \nJM1     34.0                        0.027413  ...    2.478670       2.192126   \nJR1     34.0                       -0.083395  ...    1.767363       1.844408   \nJS1     57.0                       -0.095143  ...    2.235044       2.277080   \nKK1     11.5                        0.239251  ...    1.844331       1.939742   \nKP1     38.0                        0.126382  ...    1.736222       1.624049   \nKR1     25.0                        0.257641  ...    2.016530       1.970034   \nLB1     12.0                        0.365456  ...    1.764201       1.803203   \nMB1     39.0                       -0.234160  ...    1.809708       1.487029   \nMG1     57.0                       -0.384115  ...    1.588440       1.480362   \nMP1     45.0                       -0.486909  ...    2.122926       2.598077   \nNW1     26.0                        0.496930  ...    3.079307       2.663945   \nSK1     39.0                       -0.078838  ...    2.246061       2.085018   \nSM2     44.0                        0.193566  ...    2.368565       2.319486   \nSR1     21.5                        0.142195  ...    1.680886       1.754788   \nUP1     46.0                        0.057457  ...    1.941150       2.477219   \nWK1     62.0                       -0.040481  ...    1.727043       1.453272   \nZN1     55.0                       -0.387589  ...    2.155943       1.805936   \n\n                                                                               \\\nepoch_                                                                          \nregion_agg                   cor_RH_DorsAttn cor_RH_Limbic cor_RH_SalVentAttn   \nSubject Score_median_first20                                                    \nAB1     14.0                        2.160562      0.863538           1.422210   \nAG1     47.0                        2.485330      1.140412           2.213701   \nAH1     41.5                        1.966960      0.732602           1.522058   \nAM1     27.0                        2.141479      1.207785           2.172474   \nAP1     17.0                        2.251900      1.140876           1.687866   \nAV1     48.5                        2.175813      1.320186           1.759614   \nBN1     75.0                        2.015882      1.446961           2.021826   \nCD1     15.0                        2.332097      1.320598           1.933629   \nCG1     32.0                        2.206035      1.227193           2.005296   \nCH1     39.0                        2.101218      1.017975           1.840660   \nCK1     34.0                        2.053382      0.788506           1.592865   \nCM1     29.5                        1.917768      1.043854           1.779677   \nCM2     73.0                        2.425416      1.421685           1.916807   \nEH1     21.0                        2.438391      1.240941           2.022860   \nEK1     12.0                        2.122259      1.363259           2.042452   \nHK1     17.0                        2.036788      0.782307           1.652856   \nHS1     67.0                        1.796140      0.959187           1.704678   \nJC1     12.5                        2.062122      1.285489           1.537065   \nJH1     8.5                         2.472577      1.306469           2.494458   \nJM1     34.0                        2.661035      1.260877           1.729108   \nJR1     34.0                        2.340844      1.113321           1.676130   \nJS1     57.0                        2.747927      1.405836           2.198853   \nKK1     11.5                        2.342958      1.196451           1.699696   \nKP1     38.0                        1.833292      0.967197           1.754282   \nKR1     25.0                        2.122277      1.061288           1.834982   \nLB1     12.0                        1.943513      1.059673           1.532155   \nMB1     39.0                        1.875054      0.960740           1.361371   \nMG1     57.0                        1.638124      0.962248           1.373220   \nMP1     45.0                        2.372456      1.378973           2.369778   \nNW1     26.0                        2.993097      1.766643           2.633523   \nSK1     39.0                        2.281964      1.093341           1.855895   \nSM2     44.0                        3.058632      1.231436           2.313356   \nSR1     21.5                        1.948046      0.829129           1.603257   \nUP1     46.0                        2.345197      1.137999           1.845406   \nWK1     62.0                        1.633706      1.006814           1.713639   \nZN1     55.0                        1.947398      1.077923           1.944697   \n\n                                                                            \\\nepoch_                                                                       \nregion_agg                   cor_RH_SomMot cor_RH_Vis subc_Left subc_Right   \nSubject Score_median_first20                                                 \nAB1     14.0                      1.916295   2.003551  0.771981   0.796981   \nAG1     47.0                      2.169538   2.587536  0.745009   0.749574   \nAH1     41.5                      1.714008   1.818767  0.547487   0.553544   \nAM1     27.0                      1.945818   2.561212  0.683115   0.652229   \nAP1     17.0                      1.951376   2.173467  0.897872   0.850509   \nAV1     48.5                      1.811613   2.122644  0.793224   0.761789   \nBN1     75.0                      1.898347   3.550143  1.100902   1.116964   \nCD1     15.0                      2.319423   2.608868  0.798232   0.860821   \nCG1     32.0                      1.990659   2.628626  0.945395   0.928960   \nCH1     39.0                      1.822882   1.800403  0.740360   0.711458   \nCK1     34.0                      1.981506   2.190200  0.624014   0.559237   \nCM1     29.5                      1.965310   2.384994  0.639517   0.778450   \nCM2     73.0                      1.834765   2.582323  0.663826   0.742096   \nEH1     21.0                      1.749947   2.857961  0.838048   0.705073   \nEK1     12.0                      1.750775   2.079249  1.069446   0.997267   \nHK1     17.0                      1.783014   1.970913  0.655538   0.538979   \nHS1     67.0                      2.418599   2.570555  0.651496   0.533009   \nJC1     12.5                      1.875158   2.390729  0.851868   0.802638   \nJH1     8.5                       2.319943   2.074895  1.041253   1.024920   \nJM1     34.0                      1.861761   1.587026  1.256556   1.091728   \nJR1     34.0                      1.741595   2.407909  0.932248   0.843510   \nJS1     57.0                      3.195862   2.860114  0.852203   0.809016   \nKK1     11.5                      2.183249   1.957275  0.683781   0.654229   \nKP1     38.0                      1.623096   2.350881  0.771450   0.716038   \nKR1     25.0                      2.129779   2.158371  0.621210   0.747211   \nLB1     12.0                      1.502479   2.226467  0.758493   0.740046   \nMB1     39.0                      1.568102   2.218430  0.751340   0.673188   \nMG1     57.0                      1.690456   1.676091  0.580182   0.526279   \nMP1     45.0                      2.705829   2.800093  0.856064   0.862693   \nNW1     26.0                      3.045875   2.619249  0.956131   0.881309   \nSK1     39.0                      2.148046   2.295968  0.747645   0.724677   \nSM2     44.0                      2.359852   2.787416  0.844175   0.771756   \nSR1     21.5                      2.127442   2.237622  0.505732   0.526616   \nUP1     46.0                      2.196156   2.911876  0.910662   0.953792   \nWK1     62.0                      1.724839   1.926901  0.593836   0.605323   \nZN1     55.0                      2.245211   2.400097  0.873998   0.920408   \n\n                                          \nepoch_                                    \nregion_agg                   subc_Vermis  \nSubject Score_median_first20              \nAB1     14.0                    0.845461  \nAG1     47.0                    0.839705  \nAH1     41.5                    0.593992  \nAM1     27.0                    0.697552  \nAP1     17.0                    0.879986  \nAV1     48.5                    0.989130  \nBN1     75.0                    1.013991  \nCD1     15.0                    1.144511  \nCG1     32.0                    0.836702  \nCH1     39.0                    0.766755  \nCK1     34.0                    0.503816  \nCM1     29.5                    0.915022  \nCM2     73.0                    0.416214  \nEH1     21.0                    1.037394  \nEK1     12.0                    0.930993  \nHK1     17.0                    0.522497  \nHS1     67.0                    0.629259  \nJC1     12.5                    0.671588  \nJH1     8.5                     0.842064  \nJM1     34.0                    0.995402  \nJR1     34.0                    0.939630  \nJS1     57.0                    0.767970  \nKK1     11.5                    0.587645  \nKP1     38.0                    0.483066  \nKR1     25.0                    0.775904  \nLB1     12.0                    0.639216  \nMB1     39.0                    0.556428  \nMG1     57.0                    0.386052  \nMP1     45.0                    0.950355  \nNW1     26.0                    0.802078  \nSK1     39.0                    0.638925  \nSM2     44.0                    0.709416  \nSR1     21.5                    0.540050  \nUP1     46.0                    0.818327  \nWK1     62.0                    0.420779  \nZN1     55.0                    0.869525  \n\n[36 rows x 85 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th colspan=\"21\" halign=\"left\">value</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>epoch_</th>\n      <th colspan=\"10\" halign=\"left\">b2e</th>\n      <th>...</th>\n      <th colspan=\"10\" halign=\"left\">late</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>region_agg</th>\n      <th>cor_LH_Cont</th>\n      <th>cor_LH_Default</th>\n      <th>cor_LH_DorsAttn</th>\n      <th>cor_LH_Limbic</th>\n      <th>cor_LH_SalVentAttn</th>\n      <th>cor_LH_SomMot</th>\n      <th>cor_LH_Vis</th>\n      <th>cor_RH_Cont</th>\n      <th>cor_RH_Default</th>\n      <th>cor_RH_DorsAttn</th>\n      <th>...</th>\n      <th>cor_RH_Cont</th>\n      <th>cor_RH_Default</th>\n      <th>cor_RH_DorsAttn</th>\n      <th>cor_RH_Limbic</th>\n      <th>cor_RH_SalVentAttn</th>\n      <th>cor_RH_SomMot</th>\n      <th>cor_RH_Vis</th>\n      <th>subc_Left</th>\n      <th>subc_Right</th>\n      <th>subc_Vermis</th>\n    </tr>\n    <tr>\n      <th>Subject</th>\n      <th>Score_median_first20</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AB1</th>\n      <th>14.0</th>\n      <td>0.096832</td>\n      <td>0.285795</td>\n      <td>0.336393</td>\n      <td>0.191118</td>\n      <td>0.141011</td>\n      <td>0.004024</td>\n      <td>0.448154</td>\n      <td>-0.133301</td>\n      <td>-0.009500</td>\n      <td>0.718707</td>\n      <td>...</td>\n      <td>1.718728</td>\n      <td>1.976566</td>\n      <td>2.160562</td>\n      <td>0.863538</td>\n      <td>1.422210</td>\n      <td>1.916295</td>\n      <td>2.003551</td>\n      <td>0.771981</td>\n      <td>0.796981</td>\n      <td>0.845461</td>\n    </tr>\n    <tr>\n      <th>AG1</th>\n      <th>47.0</th>\n      <td>0.023247</td>\n      <td>0.134270</td>\n      <td>0.016678</td>\n      <td>-0.172098</td>\n      <td>0.584519</td>\n      <td>0.466026</td>\n      <td>-0.106237</td>\n      <td>0.255863</td>\n      <td>-0.195050</td>\n      <td>0.175015</td>\n      <td>...</td>\n      <td>2.118806</td>\n      <td>1.950530</td>\n      <td>2.485330</td>\n      <td>1.140412</td>\n      <td>2.213701</td>\n      <td>2.169538</td>\n      <td>2.587536</td>\n      <td>0.745009</td>\n      <td>0.749574</td>\n      <td>0.839705</td>\n    </tr>\n    <tr>\n      <th>AH1</th>\n      <th>41.5</th>\n      <td>-0.323308</td>\n      <td>-0.037860</td>\n      <td>-0.711544</td>\n      <td>-0.305593</td>\n      <td>-0.227577</td>\n      <td>0.013477</td>\n      <td>-0.423758</td>\n      <td>-0.185166</td>\n      <td>-0.366852</td>\n      <td>-0.494792</td>\n      <td>...</td>\n      <td>1.469235</td>\n      <td>1.521187</td>\n      <td>1.966960</td>\n      <td>0.732602</td>\n      <td>1.522058</td>\n      <td>1.714008</td>\n      <td>1.818767</td>\n      <td>0.547487</td>\n      <td>0.553544</td>\n      <td>0.593992</td>\n    </tr>\n    <tr>\n      <th>AM1</th>\n      <th>27.0</th>\n      <td>0.175577</td>\n      <td>0.412751</td>\n      <td>0.288554</td>\n      <td>0.048740</td>\n      <td>0.684970</td>\n      <td>0.351564</td>\n      <td>0.214291</td>\n      <td>0.090384</td>\n      <td>0.558743</td>\n      <td>-0.080070</td>\n      <td>...</td>\n      <td>1.986381</td>\n      <td>2.447330</td>\n      <td>2.141479</td>\n      <td>1.207785</td>\n      <td>2.172474</td>\n      <td>1.945818</td>\n      <td>2.561212</td>\n      <td>0.683115</td>\n      <td>0.652229</td>\n      <td>0.697552</td>\n    </tr>\n    <tr>\n      <th>AP1</th>\n      <th>17.0</th>\n      <td>0.467508</td>\n      <td>0.169592</td>\n      <td>-0.193752</td>\n      <td>-0.240978</td>\n      <td>0.309113</td>\n      <td>-0.226975</td>\n      <td>0.522462</td>\n      <td>-0.106493</td>\n      <td>-0.040279</td>\n      <td>-0.234033</td>\n      <td>...</td>\n      <td>1.900803</td>\n      <td>1.932110</td>\n      <td>2.251900</td>\n      <td>1.140876</td>\n      <td>1.687866</td>\n      <td>1.951376</td>\n      <td>2.173467</td>\n      <td>0.897872</td>\n      <td>0.850509</td>\n      <td>0.879986</td>\n    </tr>\n    <tr>\n      <th>AV1</th>\n      <th>48.5</th>\n      <td>-0.038907</td>\n      <td>-0.278500</td>\n      <td>0.676340</td>\n      <td>-0.182464</td>\n      <td>0.137662</td>\n      <td>0.267408</td>\n      <td>-0.640726</td>\n      <td>-0.031024</td>\n      <td>-0.274715</td>\n      <td>0.355416</td>\n      <td>...</td>\n      <td>1.825501</td>\n      <td>1.970395</td>\n      <td>2.175813</td>\n      <td>1.320186</td>\n      <td>1.759614</td>\n      <td>1.811613</td>\n      <td>2.122644</td>\n      <td>0.793224</td>\n      <td>0.761789</td>\n      <td>0.989130</td>\n    </tr>\n    <tr>\n      <th>BN1</th>\n      <th>75.0</th>\n      <td>0.056349</td>\n      <td>-0.016979</td>\n      <td>-0.353304</td>\n      <td>-0.461987</td>\n      <td>-0.095968</td>\n      <td>-0.302246</td>\n      <td>-0.727874</td>\n      <td>-0.240385</td>\n      <td>-0.361364</td>\n      <td>-0.243420</td>\n      <td>...</td>\n      <td>1.828686</td>\n      <td>2.417146</td>\n      <td>2.015882</td>\n      <td>1.446961</td>\n      <td>2.021826</td>\n      <td>1.898347</td>\n      <td>3.550143</td>\n      <td>1.100902</td>\n      <td>1.116964</td>\n      <td>1.013991</td>\n    </tr>\n    <tr>\n      <th>CD1</th>\n      <th>15.0</th>\n      <td>0.051840</td>\n      <td>-0.055279</td>\n      <td>-0.196941</td>\n      <td>-0.095444</td>\n      <td>-0.239501</td>\n      <td>-0.159295</td>\n      <td>0.439867</td>\n      <td>0.079581</td>\n      <td>-0.485805</td>\n      <td>-0.051421</td>\n      <td>...</td>\n      <td>1.917480</td>\n      <td>1.779552</td>\n      <td>2.332097</td>\n      <td>1.320598</td>\n      <td>1.933629</td>\n      <td>2.319423</td>\n      <td>2.608868</td>\n      <td>0.798232</td>\n      <td>0.860821</td>\n      <td>1.144511</td>\n    </tr>\n    <tr>\n      <th>CG1</th>\n      <th>32.0</th>\n      <td>0.108409</td>\n      <td>0.024720</td>\n      <td>0.175183</td>\n      <td>-0.050737</td>\n      <td>-0.377077</td>\n      <td>-0.248930</td>\n      <td>-0.497208</td>\n      <td>0.301012</td>\n      <td>-0.195419</td>\n      <td>0.396552</td>\n      <td>...</td>\n      <td>1.843634</td>\n      <td>1.816909</td>\n      <td>2.206035</td>\n      <td>1.227193</td>\n      <td>2.005296</td>\n      <td>1.990659</td>\n      <td>2.628626</td>\n      <td>0.945395</td>\n      <td>0.928960</td>\n      <td>0.836702</td>\n    </tr>\n    <tr>\n      <th>CH1</th>\n      <th>39.0</th>\n      <td>0.177342</td>\n      <td>-0.087421</td>\n      <td>-0.016997</td>\n      <td>-0.433329</td>\n      <td>-0.306632</td>\n      <td>-0.416012</td>\n      <td>-0.262217</td>\n      <td>0.006448</td>\n      <td>-0.122991</td>\n      <td>-0.126323</td>\n      <td>...</td>\n      <td>1.691757</td>\n      <td>1.820045</td>\n      <td>2.101218</td>\n      <td>1.017975</td>\n      <td>1.840660</td>\n      <td>1.822882</td>\n      <td>1.800403</td>\n      <td>0.740360</td>\n      <td>0.711458</td>\n      <td>0.766755</td>\n    </tr>\n    <tr>\n      <th>CK1</th>\n      <th>34.0</th>\n      <td>0.086924</td>\n      <td>0.137110</td>\n      <td>0.299040</td>\n      <td>-0.123841</td>\n      <td>0.162907</td>\n      <td>0.514591</td>\n      <td>0.492125</td>\n      <td>0.150974</td>\n      <td>0.323942</td>\n      <td>0.202864</td>\n      <td>...</td>\n      <td>1.802336</td>\n      <td>1.631477</td>\n      <td>2.053382</td>\n      <td>0.788506</td>\n      <td>1.592865</td>\n      <td>1.981506</td>\n      <td>2.190200</td>\n      <td>0.624014</td>\n      <td>0.559237</td>\n      <td>0.503816</td>\n    </tr>\n    <tr>\n      <th>CM1</th>\n      <th>29.5</th>\n      <td>0.210084</td>\n      <td>-0.287603</td>\n      <td>0.282841</td>\n      <td>0.191320</td>\n      <td>-0.181049</td>\n      <td>0.408398</td>\n      <td>-0.415814</td>\n      <td>0.109322</td>\n      <td>-0.269060</td>\n      <td>-0.058011</td>\n      <td>...</td>\n      <td>1.714557</td>\n      <td>1.964727</td>\n      <td>1.917768</td>\n      <td>1.043854</td>\n      <td>1.779677</td>\n      <td>1.965310</td>\n      <td>2.384994</td>\n      <td>0.639517</td>\n      <td>0.778450</td>\n      <td>0.915022</td>\n    </tr>\n    <tr>\n      <th>CM2</th>\n      <th>73.0</th>\n      <td>0.268694</td>\n      <td>0.294601</td>\n      <td>-0.253968</td>\n      <td>0.040571</td>\n      <td>-0.332977</td>\n      <td>-0.143175</td>\n      <td>-0.212977</td>\n      <td>0.415070</td>\n      <td>0.127135</td>\n      <td>0.055575</td>\n      <td>...</td>\n      <td>2.143757</td>\n      <td>1.788549</td>\n      <td>2.425416</td>\n      <td>1.421685</td>\n      <td>1.916807</td>\n      <td>1.834765</td>\n      <td>2.582323</td>\n      <td>0.663826</td>\n      <td>0.742096</td>\n      <td>0.416214</td>\n    </tr>\n    <tr>\n      <th>EH1</th>\n      <th>21.0</th>\n      <td>0.387339</td>\n      <td>0.257275</td>\n      <td>0.291328</td>\n      <td>0.031696</td>\n      <td>0.448034</td>\n      <td>0.225895</td>\n      <td>-0.088108</td>\n      <td>0.405360</td>\n      <td>0.145292</td>\n      <td>0.361376</td>\n      <td>...</td>\n      <td>2.139039</td>\n      <td>2.438279</td>\n      <td>2.438391</td>\n      <td>1.240941</td>\n      <td>2.022860</td>\n      <td>1.749947</td>\n      <td>2.857961</td>\n      <td>0.838048</td>\n      <td>0.705073</td>\n      <td>1.037394</td>\n    </tr>\n    <tr>\n      <th>EK1</th>\n      <th>12.0</th>\n      <td>0.013718</td>\n      <td>-0.145739</td>\n      <td>0.582988</td>\n      <td>0.007816</td>\n      <td>0.292638</td>\n      <td>0.670939</td>\n      <td>0.623340</td>\n      <td>-0.264783</td>\n      <td>0.040501</td>\n      <td>0.387089</td>\n      <td>...</td>\n      <td>1.998436</td>\n      <td>1.958971</td>\n      <td>2.122259</td>\n      <td>1.363259</td>\n      <td>2.042452</td>\n      <td>1.750775</td>\n      <td>2.079249</td>\n      <td>1.069446</td>\n      <td>0.997267</td>\n      <td>0.930993</td>\n    </tr>\n    <tr>\n      <th>HK1</th>\n      <th>17.0</th>\n      <td>0.321216</td>\n      <td>0.582425</td>\n      <td>0.415549</td>\n      <td>0.005094</td>\n      <td>0.273420</td>\n      <td>0.180396</td>\n      <td>0.045845</td>\n      <td>0.132566</td>\n      <td>0.025067</td>\n      <td>0.036989</td>\n      <td>...</td>\n      <td>2.017204</td>\n      <td>1.528434</td>\n      <td>2.036788</td>\n      <td>0.782307</td>\n      <td>1.652856</td>\n      <td>1.783014</td>\n      <td>1.970913</td>\n      <td>0.655538</td>\n      <td>0.538979</td>\n      <td>0.522497</td>\n    </tr>\n    <tr>\n      <th>HS1</th>\n      <th>67.0</th>\n      <td>0.170226</td>\n      <td>-0.168491</td>\n      <td>0.353160</td>\n      <td>-0.060915</td>\n      <td>0.104842</td>\n      <td>0.046738</td>\n      <td>0.540557</td>\n      <td>0.363904</td>\n      <td>-0.218475</td>\n      <td>0.434677</td>\n      <td>...</td>\n      <td>1.749179</td>\n      <td>1.640362</td>\n      <td>1.796140</td>\n      <td>0.959187</td>\n      <td>1.704678</td>\n      <td>2.418599</td>\n      <td>2.570555</td>\n      <td>0.651496</td>\n      <td>0.533009</td>\n      <td>0.629259</td>\n    </tr>\n    <tr>\n      <th>JC1</th>\n      <th>12.5</th>\n      <td>0.441986</td>\n      <td>0.234091</td>\n      <td>0.437221</td>\n      <td>-0.171470</td>\n      <td>0.216009</td>\n      <td>-0.057136</td>\n      <td>0.280812</td>\n      <td>-0.175299</td>\n      <td>0.204475</td>\n      <td>0.016970</td>\n      <td>...</td>\n      <td>2.017339</td>\n      <td>1.882053</td>\n      <td>2.062122</td>\n      <td>1.285489</td>\n      <td>1.537065</td>\n      <td>1.875158</td>\n      <td>2.390729</td>\n      <td>0.851868</td>\n      <td>0.802638</td>\n      <td>0.671588</td>\n    </tr>\n    <tr>\n      <th>JH1</th>\n      <th>8.5</th>\n      <td>-0.026666</td>\n      <td>0.106128</td>\n      <td>0.104092</td>\n      <td>0.458677</td>\n      <td>-0.207485</td>\n      <td>-0.334236</td>\n      <td>0.130646</td>\n      <td>-0.122474</td>\n      <td>0.023361</td>\n      <td>0.134300</td>\n      <td>...</td>\n      <td>1.998840</td>\n      <td>2.415192</td>\n      <td>2.472577</td>\n      <td>1.306469</td>\n      <td>2.494458</td>\n      <td>2.319943</td>\n      <td>2.074895</td>\n      <td>1.041253</td>\n      <td>1.024920</td>\n      <td>0.842064</td>\n    </tr>\n    <tr>\n      <th>JM1</th>\n      <th>34.0</th>\n      <td>-0.281974</td>\n      <td>-0.076701</td>\n      <td>0.006779</td>\n      <td>-0.074407</td>\n      <td>0.129751</td>\n      <td>0.341420</td>\n      <td>-0.310428</td>\n      <td>-0.134713</td>\n      <td>-0.202092</td>\n      <td>0.027413</td>\n      <td>...</td>\n      <td>2.478670</td>\n      <td>2.192126</td>\n      <td>2.661035</td>\n      <td>1.260877</td>\n      <td>1.729108</td>\n      <td>1.861761</td>\n      <td>1.587026</td>\n      <td>1.256556</td>\n      <td>1.091728</td>\n      <td>0.995402</td>\n    </tr>\n    <tr>\n      <th>JR1</th>\n      <th>34.0</th>\n      <td>-0.072512</td>\n      <td>0.251876</td>\n      <td>-0.125334</td>\n      <td>0.136915</td>\n      <td>-0.049604</td>\n      <td>-0.354429</td>\n      <td>-0.095955</td>\n      <td>0.192945</td>\n      <td>-0.159106</td>\n      <td>-0.083395</td>\n      <td>...</td>\n      <td>1.767363</td>\n      <td>1.844408</td>\n      <td>2.340844</td>\n      <td>1.113321</td>\n      <td>1.676130</td>\n      <td>1.741595</td>\n      <td>2.407909</td>\n      <td>0.932248</td>\n      <td>0.843510</td>\n      <td>0.939630</td>\n    </tr>\n    <tr>\n      <th>JS1</th>\n      <th>57.0</th>\n      <td>0.057821</td>\n      <td>0.119935</td>\n      <td>0.076817</td>\n      <td>-0.161564</td>\n      <td>0.178161</td>\n      <td>0.385102</td>\n      <td>-0.150975</td>\n      <td>-0.237924</td>\n      <td>-0.045934</td>\n      <td>-0.095143</td>\n      <td>...</td>\n      <td>2.235044</td>\n      <td>2.277080</td>\n      <td>2.747927</td>\n      <td>1.405836</td>\n      <td>2.198853</td>\n      <td>3.195862</td>\n      <td>2.860114</td>\n      <td>0.852203</td>\n      <td>0.809016</td>\n      <td>0.767970</td>\n    </tr>\n    <tr>\n      <th>KK1</th>\n      <th>11.5</th>\n      <td>-0.025902</td>\n      <td>0.484708</td>\n      <td>0.069643</td>\n      <td>-0.103952</td>\n      <td>0.453943</td>\n      <td>0.125557</td>\n      <td>0.744675</td>\n      <td>0.217756</td>\n      <td>0.099546</td>\n      <td>0.239251</td>\n      <td>...</td>\n      <td>1.844331</td>\n      <td>1.939742</td>\n      <td>2.342958</td>\n      <td>1.196451</td>\n      <td>1.699696</td>\n      <td>2.183249</td>\n      <td>1.957275</td>\n      <td>0.683781</td>\n      <td>0.654229</td>\n      <td>0.587645</td>\n    </tr>\n    <tr>\n      <th>KP1</th>\n      <th>38.0</th>\n      <td>0.131821</td>\n      <td>0.172627</td>\n      <td>0.242722</td>\n      <td>-0.000527</td>\n      <td>0.143837</td>\n      <td>-0.173275</td>\n      <td>0.350981</td>\n      <td>0.013444</td>\n      <td>-0.083172</td>\n      <td>0.126382</td>\n      <td>...</td>\n      <td>1.736222</td>\n      <td>1.624049</td>\n      <td>1.833292</td>\n      <td>0.967197</td>\n      <td>1.754282</td>\n      <td>1.623096</td>\n      <td>2.350881</td>\n      <td>0.771450</td>\n      <td>0.716038</td>\n      <td>0.483066</td>\n    </tr>\n    <tr>\n      <th>KR1</th>\n      <th>25.0</th>\n      <td>0.492554</td>\n      <td>0.689035</td>\n      <td>0.391546</td>\n      <td>0.061062</td>\n      <td>0.117760</td>\n      <td>0.363120</td>\n      <td>0.007332</td>\n      <td>0.464407</td>\n      <td>0.486510</td>\n      <td>0.257641</td>\n      <td>...</td>\n      <td>2.016530</td>\n      <td>1.970034</td>\n      <td>2.122277</td>\n      <td>1.061288</td>\n      <td>1.834982</td>\n      <td>2.129779</td>\n      <td>2.158371</td>\n      <td>0.621210</td>\n      <td>0.747211</td>\n      <td>0.775904</td>\n    </tr>\n    <tr>\n      <th>LB1</th>\n      <th>12.0</th>\n      <td>-0.088604</td>\n      <td>0.422639</td>\n      <td>0.162973</td>\n      <td>-0.061952</td>\n      <td>0.070794</td>\n      <td>-0.100209</td>\n      <td>0.464483</td>\n      <td>0.166278</td>\n      <td>-0.112810</td>\n      <td>0.365456</td>\n      <td>...</td>\n      <td>1.764201</td>\n      <td>1.803203</td>\n      <td>1.943513</td>\n      <td>1.059673</td>\n      <td>1.532155</td>\n      <td>1.502479</td>\n      <td>2.226467</td>\n      <td>0.758493</td>\n      <td>0.740046</td>\n      <td>0.639216</td>\n    </tr>\n    <tr>\n      <th>MB1</th>\n      <th>39.0</th>\n      <td>-0.056847</td>\n      <td>0.236683</td>\n      <td>-0.310333</td>\n      <td>-0.420323</td>\n      <td>0.395842</td>\n      <td>-0.279680</td>\n      <td>-0.298043</td>\n      <td>-0.335835</td>\n      <td>-0.135304</td>\n      <td>-0.234160</td>\n      <td>...</td>\n      <td>1.809708</td>\n      <td>1.487029</td>\n      <td>1.875054</td>\n      <td>0.960740</td>\n      <td>1.361371</td>\n      <td>1.568102</td>\n      <td>2.218430</td>\n      <td>0.751340</td>\n      <td>0.673188</td>\n      <td>0.556428</td>\n    </tr>\n    <tr>\n      <th>MG1</th>\n      <th>57.0</th>\n      <td>-0.252071</td>\n      <td>-0.571489</td>\n      <td>-0.340796</td>\n      <td>-0.205559</td>\n      <td>-0.285394</td>\n      <td>-0.190710</td>\n      <td>-0.268854</td>\n      <td>-0.198966</td>\n      <td>-0.395889</td>\n      <td>-0.384115</td>\n      <td>...</td>\n      <td>1.588440</td>\n      <td>1.480362</td>\n      <td>1.638124</td>\n      <td>0.962248</td>\n      <td>1.373220</td>\n      <td>1.690456</td>\n      <td>1.676091</td>\n      <td>0.580182</td>\n      <td>0.526279</td>\n      <td>0.386052</td>\n    </tr>\n    <tr>\n      <th>MP1</th>\n      <th>45.0</th>\n      <td>0.038101</td>\n      <td>-0.436873</td>\n      <td>-0.311200</td>\n      <td>-0.566607</td>\n      <td>-0.674823</td>\n      <td>-0.209500</td>\n      <td>-1.388130</td>\n      <td>-0.376981</td>\n      <td>-0.475804</td>\n      <td>-0.486909</td>\n      <td>...</td>\n      <td>2.122926</td>\n      <td>2.598077</td>\n      <td>2.372456</td>\n      <td>1.378973</td>\n      <td>2.369778</td>\n      <td>2.705829</td>\n      <td>2.800093</td>\n      <td>0.856064</td>\n      <td>0.862693</td>\n      <td>0.950355</td>\n    </tr>\n    <tr>\n      <th>NW1</th>\n      <th>26.0</th>\n      <td>-0.284932</td>\n      <td>-0.155696</td>\n      <td>0.231738</td>\n      <td>-0.321735</td>\n      <td>0.158029</td>\n      <td>0.738793</td>\n      <td>-0.509783</td>\n      <td>-0.087884</td>\n      <td>-0.138356</td>\n      <td>0.496930</td>\n      <td>...</td>\n      <td>3.079307</td>\n      <td>2.663945</td>\n      <td>2.993097</td>\n      <td>1.766643</td>\n      <td>2.633523</td>\n      <td>3.045875</td>\n      <td>2.619249</td>\n      <td>0.956131</td>\n      <td>0.881309</td>\n      <td>0.802078</td>\n    </tr>\n    <tr>\n      <th>SK1</th>\n      <th>39.0</th>\n      <td>0.353440</td>\n      <td>0.344001</td>\n      <td>-0.212267</td>\n      <td>0.011286</td>\n      <td>-0.277125</td>\n      <td>-0.489517</td>\n      <td>-0.061978</td>\n      <td>0.330996</td>\n      <td>0.195117</td>\n      <td>-0.078838</td>\n      <td>...</td>\n      <td>2.246061</td>\n      <td>2.085018</td>\n      <td>2.281964</td>\n      <td>1.093341</td>\n      <td>1.855895</td>\n      <td>2.148046</td>\n      <td>2.295968</td>\n      <td>0.747645</td>\n      <td>0.724677</td>\n      <td>0.638925</td>\n    </tr>\n    <tr>\n      <th>SM2</th>\n      <th>44.0</th>\n      <td>0.156058</td>\n      <td>0.257919</td>\n      <td>0.384763</td>\n      <td>0.080896</td>\n      <td>0.152030</td>\n      <td>0.202089</td>\n      <td>0.151459</td>\n      <td>-0.094142</td>\n      <td>-0.164098</td>\n      <td>0.193566</td>\n      <td>...</td>\n      <td>2.368565</td>\n      <td>2.319486</td>\n      <td>3.058632</td>\n      <td>1.231436</td>\n      <td>2.313356</td>\n      <td>2.359852</td>\n      <td>2.787416</td>\n      <td>0.844175</td>\n      <td>0.771756</td>\n      <td>0.709416</td>\n    </tr>\n    <tr>\n      <th>SR1</th>\n      <th>21.5</th>\n      <td>-0.047300</td>\n      <td>-0.070251</td>\n      <td>-0.042661</td>\n      <td>-0.290926</td>\n      <td>-0.490961</td>\n      <td>0.141022</td>\n      <td>-0.125938</td>\n      <td>-0.084477</td>\n      <td>-0.511392</td>\n      <td>0.142195</td>\n      <td>...</td>\n      <td>1.680886</td>\n      <td>1.754788</td>\n      <td>1.948046</td>\n      <td>0.829129</td>\n      <td>1.603257</td>\n      <td>2.127442</td>\n      <td>2.237622</td>\n      <td>0.505732</td>\n      <td>0.526616</td>\n      <td>0.540050</td>\n    </tr>\n    <tr>\n      <th>UP1</th>\n      <th>46.0</th>\n      <td>0.141589</td>\n      <td>0.151221</td>\n      <td>0.142272</td>\n      <td>0.022815</td>\n      <td>0.133355</td>\n      <td>0.163306</td>\n      <td>1.118926</td>\n      <td>-0.426311</td>\n      <td>-0.036800</td>\n      <td>0.057457</td>\n      <td>...</td>\n      <td>1.941150</td>\n      <td>2.477219</td>\n      <td>2.345197</td>\n      <td>1.137999</td>\n      <td>1.845406</td>\n      <td>2.196156</td>\n      <td>2.911876</td>\n      <td>0.910662</td>\n      <td>0.953792</td>\n      <td>0.818327</td>\n    </tr>\n    <tr>\n      <th>WK1</th>\n      <th>62.0</th>\n      <td>0.068048</td>\n      <td>0.233606</td>\n      <td>0.053965</td>\n      <td>0.047418</td>\n      <td>-0.042987</td>\n      <td>0.142878</td>\n      <td>-0.112929</td>\n      <td>0.186223</td>\n      <td>0.062975</td>\n      <td>-0.040481</td>\n      <td>...</td>\n      <td>1.727043</td>\n      <td>1.453272</td>\n      <td>1.633706</td>\n      <td>1.006814</td>\n      <td>1.713639</td>\n      <td>1.724839</td>\n      <td>1.926901</td>\n      <td>0.593836</td>\n      <td>0.605323</td>\n      <td>0.420779</td>\n    </tr>\n    <tr>\n      <th>ZN1</th>\n      <th>55.0</th>\n      <td>0.146824</td>\n      <td>-0.541973</td>\n      <td>-0.176711</td>\n      <td>-0.314063</td>\n      <td>0.160933</td>\n      <td>-0.103045</td>\n      <td>-0.715399</td>\n      <td>-0.297575</td>\n      <td>-0.164273</td>\n      <td>-0.387589</td>\n      <td>...</td>\n      <td>2.155943</td>\n      <td>1.805936</td>\n      <td>1.947398</td>\n      <td>1.077923</td>\n      <td>1.944697</td>\n      <td>2.245211</td>\n      <td>2.400097</td>\n      <td>0.873998</td>\n      <td>0.920408</td>\n      <td>0.869525</td>\n    </tr>\n  </tbody>\n</table>\n<p>36 rows Ã— 85 columns</p>\n</div>"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfecc = dfecc.pivot_table(columns=['epoch_', 'region_agg'],\n",
    "               values=['value'],\n",
    "               index=['Subject', 'Score_median_first20'])\n",
    "\n",
    "dfecc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "Subject  Score_median_first20\nAB1      14.0                    0.096832\nAG1      47.0                    0.023247\nAH1      41.5                   -0.323308\nAM1      27.0                    0.175577\nAP1      17.0                    0.467508\nAV1      48.5                   -0.038907\nBN1      75.0                    0.056349\nCD1      15.0                    0.051840\nCG1      32.0                    0.108409\nCH1      39.0                    0.177342\nCK1      34.0                    0.086924\nCM1      29.5                    0.210084\nCM2      73.0                    0.268694\nEH1      21.0                    0.387339\nEK1      12.0                    0.013718\nHK1      17.0                    0.321216\nHS1      67.0                    0.170226\nJC1      12.5                    0.441986\nJH1      8.5                    -0.026666\nJM1      34.0                   -0.281974\nJR1      34.0                   -0.072512\nJS1      57.0                    0.057821\nKK1      11.5                   -0.025902\nKP1      38.0                    0.131821\nKR1      25.0                    0.492554\nLB1      12.0                   -0.088604\nMB1      39.0                   -0.056847\nMG1      57.0                   -0.252071\nMP1      45.0                    0.038101\nNW1      26.0                   -0.284932\nSK1      39.0                    0.353440\nSM2      44.0                    0.156058\nSR1      21.5                   -0.047300\nUP1      46.0                    0.141589\nWK1      62.0                    0.068048\nZN1      55.0                    0.146824\nName: (value, b2e, cor_LH_Cont), dtype: float64"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfecc[('value',  'b2e', 'cor_LH_Cont')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "                                   value                                 \\\nepoch_                               b2e                                  \nregion_agg                   cor_LH_Cont cor_LH_Default cor_LH_DorsAttn   \nSubject Score_median_first20                                              \nAB1     14.0                    0.096832       0.285795        0.336393   \nAG1     47.0                    0.023247       0.134270        0.016678   \nAH1     41.5                   -0.323308      -0.037860       -0.711544   \nAM1     27.0                    0.175577       0.412751        0.288554   \nAP1     17.0                    0.467508       0.169592       -0.193752   \nAV1     48.5                   -0.038907      -0.278500        0.676340   \nBN1     75.0                    0.056349      -0.016979       -0.353304   \nCD1     15.0                    0.051840      -0.055279       -0.196941   \nCG1     32.0                    0.108409       0.024720        0.175183   \nCH1     39.0                    0.177342      -0.087421       -0.016997   \nCK1     34.0                    0.086924       0.137110        0.299040   \nCM1     29.5                    0.210084      -0.287603        0.282841   \nCM2     73.0                    0.268694       0.294601       -0.253968   \nEH1     21.0                    0.387339       0.257275        0.291328   \nEK1     12.0                    0.013718      -0.145739        0.582988   \nHK1     17.0                    0.321216       0.582425        0.415549   \nHS1     67.0                    0.170226      -0.168491        0.353160   \nJC1     12.5                    0.441986       0.234091        0.437221   \nJH1     8.5                    -0.026666       0.106128        0.104092   \nJM1     34.0                   -0.281974      -0.076701        0.006779   \nJR1     34.0                   -0.072512       0.251876       -0.125334   \nJS1     57.0                    0.057821       0.119935        0.076817   \nKK1     11.5                   -0.025902       0.484708        0.069643   \nKP1     38.0                    0.131821       0.172627        0.242722   \nKR1     25.0                    0.492554       0.689035        0.391546   \nLB1     12.0                   -0.088604       0.422639        0.162973   \nMB1     39.0                   -0.056847       0.236683       -0.310333   \nMG1     57.0                   -0.252071      -0.571489       -0.340796   \nMP1     45.0                    0.038101      -0.436873       -0.311200   \nNW1     26.0                   -0.284932      -0.155696        0.231738   \nSK1     39.0                    0.353440       0.344001       -0.212267   \nSM2     44.0                    0.156058       0.257919        0.384763   \nSR1     21.5                   -0.047300      -0.070251       -0.042661   \nUP1     46.0                    0.141589       0.151221        0.142272   \nWK1     62.0                    0.068048       0.233606        0.053965   \nZN1     55.0                    0.146824      -0.541973       -0.176711   \n\n                                                                             \\\nepoch_                                                                        \nregion_agg                   cor_LH_Limbic cor_LH_SalVentAttn cor_LH_SomMot   \nSubject Score_median_first20                                                  \nAB1     14.0                      0.191118           0.141011      0.004024   \nAG1     47.0                     -0.172098           0.584519      0.466026   \nAH1     41.5                     -0.305593          -0.227577      0.013477   \nAM1     27.0                      0.048740           0.684970      0.351564   \nAP1     17.0                     -0.240978           0.309113     -0.226975   \nAV1     48.5                     -0.182464           0.137662      0.267408   \nBN1     75.0                     -0.461987          -0.095968     -0.302246   \nCD1     15.0                     -0.095444          -0.239501     -0.159295   \nCG1     32.0                     -0.050737          -0.377077     -0.248930   \nCH1     39.0                     -0.433329          -0.306632     -0.416012   \nCK1     34.0                     -0.123841           0.162907      0.514591   \nCM1     29.5                      0.191320          -0.181049      0.408398   \nCM2     73.0                      0.040571          -0.332977     -0.143175   \nEH1     21.0                      0.031696           0.448034      0.225895   \nEK1     12.0                      0.007816           0.292638      0.670939   \nHK1     17.0                      0.005094           0.273420      0.180396   \nHS1     67.0                     -0.060915           0.104842      0.046738   \nJC1     12.5                     -0.171470           0.216009     -0.057136   \nJH1     8.5                       0.458677          -0.207485     -0.334236   \nJM1     34.0                     -0.074407           0.129751      0.341420   \nJR1     34.0                      0.136915          -0.049604     -0.354429   \nJS1     57.0                     -0.161564           0.178161      0.385102   \nKK1     11.5                     -0.103952           0.453943      0.125557   \nKP1     38.0                     -0.000527           0.143837     -0.173275   \nKR1     25.0                      0.061062           0.117760      0.363120   \nLB1     12.0                     -0.061952           0.070794     -0.100209   \nMB1     39.0                     -0.420323           0.395842     -0.279680   \nMG1     57.0                     -0.205559          -0.285394     -0.190710   \nMP1     45.0                     -0.566607          -0.674823     -0.209500   \nNW1     26.0                     -0.321735           0.158029      0.738793   \nSK1     39.0                      0.011286          -0.277125     -0.489517   \nSM2     44.0                      0.080896           0.152030      0.202089   \nSR1     21.5                     -0.290926          -0.490961      0.141022   \nUP1     46.0                      0.022815           0.133355      0.163306   \nWK1     62.0                      0.047418          -0.042987      0.142878   \nZN1     55.0                     -0.314063           0.160933     -0.103045   \n\n                                                                    \\\nepoch_                                                               \nregion_agg                   cor_LH_Vis cor_RH_Cont cor_RH_Default   \nSubject Score_median_first20                                         \nAB1     14.0                   0.448154   -0.133301      -0.009500   \nAG1     47.0                  -0.106237    0.255863      -0.195050   \nAH1     41.5                  -0.423758   -0.185166      -0.366852   \nAM1     27.0                   0.214291    0.090384       0.558743   \nAP1     17.0                   0.522462   -0.106493      -0.040279   \nAV1     48.5                  -0.640726   -0.031024      -0.274715   \nBN1     75.0                  -0.727874   -0.240385      -0.361364   \nCD1     15.0                   0.439867    0.079581      -0.485805   \nCG1     32.0                  -0.497208    0.301012      -0.195419   \nCH1     39.0                  -0.262217    0.006448      -0.122991   \nCK1     34.0                   0.492125    0.150974       0.323942   \nCM1     29.5                  -0.415814    0.109322      -0.269060   \nCM2     73.0                  -0.212977    0.415070       0.127135   \nEH1     21.0                  -0.088108    0.405360       0.145292   \nEK1     12.0                   0.623340   -0.264783       0.040501   \nHK1     17.0                   0.045845    0.132566       0.025067   \nHS1     67.0                   0.540557    0.363904      -0.218475   \nJC1     12.5                   0.280812   -0.175299       0.204475   \nJH1     8.5                    0.130646   -0.122474       0.023361   \nJM1     34.0                  -0.310428   -0.134713      -0.202092   \nJR1     34.0                  -0.095955    0.192945      -0.159106   \nJS1     57.0                  -0.150975   -0.237924      -0.045934   \nKK1     11.5                   0.744675    0.217756       0.099546   \nKP1     38.0                   0.350981    0.013444      -0.083172   \nKR1     25.0                   0.007332    0.464407       0.486510   \nLB1     12.0                   0.464483    0.166278      -0.112810   \nMB1     39.0                  -0.298043   -0.335835      -0.135304   \nMG1     57.0                  -0.268854   -0.198966      -0.395889   \nMP1     45.0                  -1.388130   -0.376981      -0.475804   \nNW1     26.0                  -0.509783   -0.087884      -0.138356   \nSK1     39.0                  -0.061978    0.330996       0.195117   \nSM2     44.0                   0.151459   -0.094142      -0.164098   \nSR1     21.5                  -0.125938   -0.084477      -0.511392   \nUP1     46.0                   1.118926   -0.426311      -0.036800   \nWK1     62.0                  -0.112929    0.186223       0.062975   \nZN1     55.0                  -0.715399   -0.297575      -0.164273   \n\n                                              ...                             \\\nepoch_                                        ...        late                  \nregion_agg                   cor_RH_DorsAttn  ... cor_RH_Cont cor_RH_Default   \nSubject Score_median_first20                  ...                              \nAB1     14.0                        0.718707  ...    1.718728       1.976566   \nAG1     47.0                        0.175015  ...    2.118806       1.950530   \nAH1     41.5                       -0.494792  ...    1.469235       1.521187   \nAM1     27.0                       -0.080070  ...    1.986381       2.447330   \nAP1     17.0                       -0.234033  ...    1.900803       1.932110   \nAV1     48.5                        0.355416  ...    1.825501       1.970395   \nBN1     75.0                       -0.243420  ...    1.828686       2.417146   \nCD1     15.0                       -0.051421  ...    1.917480       1.779552   \nCG1     32.0                        0.396552  ...    1.843634       1.816909   \nCH1     39.0                       -0.126323  ...    1.691757       1.820045   \nCK1     34.0                        0.202864  ...    1.802336       1.631477   \nCM1     29.5                       -0.058011  ...    1.714557       1.964727   \nCM2     73.0                        0.055575  ...    2.143757       1.788549   \nEH1     21.0                        0.361376  ...    2.139039       2.438279   \nEK1     12.0                        0.387089  ...    1.998436       1.958971   \nHK1     17.0                        0.036989  ...    2.017204       1.528434   \nHS1     67.0                        0.434677  ...    1.749179       1.640362   \nJC1     12.5                        0.016970  ...    2.017339       1.882053   \nJH1     8.5                         0.134300  ...    1.998840       2.415192   \nJM1     34.0                        0.027413  ...    2.478670       2.192126   \nJR1     34.0                       -0.083395  ...    1.767363       1.844408   \nJS1     57.0                       -0.095143  ...    2.235044       2.277080   \nKK1     11.5                        0.239251  ...    1.844331       1.939742   \nKP1     38.0                        0.126382  ...    1.736222       1.624049   \nKR1     25.0                        0.257641  ...    2.016530       1.970034   \nLB1     12.0                        0.365456  ...    1.764201       1.803203   \nMB1     39.0                       -0.234160  ...    1.809708       1.487029   \nMG1     57.0                       -0.384115  ...    1.588440       1.480362   \nMP1     45.0                       -0.486909  ...    2.122926       2.598077   \nNW1     26.0                        0.496930  ...    3.079307       2.663945   \nSK1     39.0                       -0.078838  ...    2.246061       2.085018   \nSM2     44.0                        0.193566  ...    2.368565       2.319486   \nSR1     21.5                        0.142195  ...    1.680886       1.754788   \nUP1     46.0                        0.057457  ...    1.941150       2.477219   \nWK1     62.0                       -0.040481  ...    1.727043       1.453272   \nZN1     55.0                       -0.387589  ...    2.155943       1.805936   \n\n                                                                               \\\nepoch_                                                                          \nregion_agg                   cor_RH_DorsAttn cor_RH_Limbic cor_RH_SalVentAttn   \nSubject Score_median_first20                                                    \nAB1     14.0                        2.160562      0.863538           1.422210   \nAG1     47.0                        2.485330      1.140412           2.213701   \nAH1     41.5                        1.966960      0.732602           1.522058   \nAM1     27.0                        2.141479      1.207785           2.172474   \nAP1     17.0                        2.251900      1.140876           1.687866   \nAV1     48.5                        2.175813      1.320186           1.759614   \nBN1     75.0                        2.015882      1.446961           2.021826   \nCD1     15.0                        2.332097      1.320598           1.933629   \nCG1     32.0                        2.206035      1.227193           2.005296   \nCH1     39.0                        2.101218      1.017975           1.840660   \nCK1     34.0                        2.053382      0.788506           1.592865   \nCM1     29.5                        1.917768      1.043854           1.779677   \nCM2     73.0                        2.425416      1.421685           1.916807   \nEH1     21.0                        2.438391      1.240941           2.022860   \nEK1     12.0                        2.122259      1.363259           2.042452   \nHK1     17.0                        2.036788      0.782307           1.652856   \nHS1     67.0                        1.796140      0.959187           1.704678   \nJC1     12.5                        2.062122      1.285489           1.537065   \nJH1     8.5                         2.472577      1.306469           2.494458   \nJM1     34.0                        2.661035      1.260877           1.729108   \nJR1     34.0                        2.340844      1.113321           1.676130   \nJS1     57.0                        2.747927      1.405836           2.198853   \nKK1     11.5                        2.342958      1.196451           1.699696   \nKP1     38.0                        1.833292      0.967197           1.754282   \nKR1     25.0                        2.122277      1.061288           1.834982   \nLB1     12.0                        1.943513      1.059673           1.532155   \nMB1     39.0                        1.875054      0.960740           1.361371   \nMG1     57.0                        1.638124      0.962248           1.373220   \nMP1     45.0                        2.372456      1.378973           2.369778   \nNW1     26.0                        2.993097      1.766643           2.633523   \nSK1     39.0                        2.281964      1.093341           1.855895   \nSM2     44.0                        3.058632      1.231436           2.313356   \nSR1     21.5                        1.948046      0.829129           1.603257   \nUP1     46.0                        2.345197      1.137999           1.845406   \nWK1     62.0                        1.633706      1.006814           1.713639   \nZN1     55.0                        1.947398      1.077923           1.944697   \n\n                                                                            \\\nepoch_                                                                       \nregion_agg                   cor_RH_SomMot cor_RH_Vis subc_Left subc_Right   \nSubject Score_median_first20                                                 \nAB1     14.0                      1.916295   2.003551  0.771981   0.796981   \nAG1     47.0                      2.169538   2.587536  0.745009   0.749574   \nAH1     41.5                      1.714008   1.818767  0.547487   0.553544   \nAM1     27.0                      1.945818   2.561212  0.683115   0.652229   \nAP1     17.0                      1.951376   2.173467  0.897872   0.850509   \nAV1     48.5                      1.811613   2.122644  0.793224   0.761789   \nBN1     75.0                      1.898347   3.550143  1.100902   1.116964   \nCD1     15.0                      2.319423   2.608868  0.798232   0.860821   \nCG1     32.0                      1.990659   2.628626  0.945395   0.928960   \nCH1     39.0                      1.822882   1.800403  0.740360   0.711458   \nCK1     34.0                      1.981506   2.190200  0.624014   0.559237   \nCM1     29.5                      1.965310   2.384994  0.639517   0.778450   \nCM2     73.0                      1.834765   2.582323  0.663826   0.742096   \nEH1     21.0                      1.749947   2.857961  0.838048   0.705073   \nEK1     12.0                      1.750775   2.079249  1.069446   0.997267   \nHK1     17.0                      1.783014   1.970913  0.655538   0.538979   \nHS1     67.0                      2.418599   2.570555  0.651496   0.533009   \nJC1     12.5                      1.875158   2.390729  0.851868   0.802638   \nJH1     8.5                       2.319943   2.074895  1.041253   1.024920   \nJM1     34.0                      1.861761   1.587026  1.256556   1.091728   \nJR1     34.0                      1.741595   2.407909  0.932248   0.843510   \nJS1     57.0                      3.195862   2.860114  0.852203   0.809016   \nKK1     11.5                      2.183249   1.957275  0.683781   0.654229   \nKP1     38.0                      1.623096   2.350881  0.771450   0.716038   \nKR1     25.0                      2.129779   2.158371  0.621210   0.747211   \nLB1     12.0                      1.502479   2.226467  0.758493   0.740046   \nMB1     39.0                      1.568102   2.218430  0.751340   0.673188   \nMG1     57.0                      1.690456   1.676091  0.580182   0.526279   \nMP1     45.0                      2.705829   2.800093  0.856064   0.862693   \nNW1     26.0                      3.045875   2.619249  0.956131   0.881309   \nSK1     39.0                      2.148046   2.295968  0.747645   0.724677   \nSM2     44.0                      2.359852   2.787416  0.844175   0.771756   \nSR1     21.5                      2.127442   2.237622  0.505732   0.526616   \nUP1     46.0                      2.196156   2.911876  0.910662   0.953792   \nWK1     62.0                      1.724839   1.926901  0.593836   0.605323   \nZN1     55.0                      2.245211   2.400097  0.873998   0.920408   \n\n                                          \nepoch_                                    \nregion_agg                   subc_Vermis  \nSubject Score_median_first20              \nAB1     14.0                    0.845461  \nAG1     47.0                    0.839705  \nAH1     41.5                    0.593992  \nAM1     27.0                    0.697552  \nAP1     17.0                    0.879986  \nAV1     48.5                    0.989130  \nBN1     75.0                    1.013991  \nCD1     15.0                    1.144511  \nCG1     32.0                    0.836702  \nCH1     39.0                    0.766755  \nCK1     34.0                    0.503816  \nCM1     29.5                    0.915022  \nCM2     73.0                    0.416214  \nEH1     21.0                    1.037394  \nEK1     12.0                    0.930993  \nHK1     17.0                    0.522497  \nHS1     67.0                    0.629259  \nJC1     12.5                    0.671588  \nJH1     8.5                     0.842064  \nJM1     34.0                    0.995402  \nJR1     34.0                    0.939630  \nJS1     57.0                    0.767970  \nKK1     11.5                    0.587645  \nKP1     38.0                    0.483066  \nKR1     25.0                    0.775904  \nLB1     12.0                    0.639216  \nMB1     39.0                    0.556428  \nMG1     57.0                    0.386052  \nMP1     45.0                    0.950355  \nNW1     26.0                    0.802078  \nSK1     39.0                    0.638925  \nSM2     44.0                    0.709416  \nSR1     21.5                    0.540050  \nUP1     46.0                    0.818327  \nWK1     62.0                    0.420779  \nZN1     55.0                    0.869525  \n\n[36 rows x 85 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th colspan=\"21\" halign=\"left\">value</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>epoch_</th>\n      <th colspan=\"10\" halign=\"left\">b2e</th>\n      <th>...</th>\n      <th colspan=\"10\" halign=\"left\">late</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>region_agg</th>\n      <th>cor_LH_Cont</th>\n      <th>cor_LH_Default</th>\n      <th>cor_LH_DorsAttn</th>\n      <th>cor_LH_Limbic</th>\n      <th>cor_LH_SalVentAttn</th>\n      <th>cor_LH_SomMot</th>\n      <th>cor_LH_Vis</th>\n      <th>cor_RH_Cont</th>\n      <th>cor_RH_Default</th>\n      <th>cor_RH_DorsAttn</th>\n      <th>...</th>\n      <th>cor_RH_Cont</th>\n      <th>cor_RH_Default</th>\n      <th>cor_RH_DorsAttn</th>\n      <th>cor_RH_Limbic</th>\n      <th>cor_RH_SalVentAttn</th>\n      <th>cor_RH_SomMot</th>\n      <th>cor_RH_Vis</th>\n      <th>subc_Left</th>\n      <th>subc_Right</th>\n      <th>subc_Vermis</th>\n    </tr>\n    <tr>\n      <th>Subject</th>\n      <th>Score_median_first20</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AB1</th>\n      <th>14.0</th>\n      <td>0.096832</td>\n      <td>0.285795</td>\n      <td>0.336393</td>\n      <td>0.191118</td>\n      <td>0.141011</td>\n      <td>0.004024</td>\n      <td>0.448154</td>\n      <td>-0.133301</td>\n      <td>-0.009500</td>\n      <td>0.718707</td>\n      <td>...</td>\n      <td>1.718728</td>\n      <td>1.976566</td>\n      <td>2.160562</td>\n      <td>0.863538</td>\n      <td>1.422210</td>\n      <td>1.916295</td>\n      <td>2.003551</td>\n      <td>0.771981</td>\n      <td>0.796981</td>\n      <td>0.845461</td>\n    </tr>\n    <tr>\n      <th>AG1</th>\n      <th>47.0</th>\n      <td>0.023247</td>\n      <td>0.134270</td>\n      <td>0.016678</td>\n      <td>-0.172098</td>\n      <td>0.584519</td>\n      <td>0.466026</td>\n      <td>-0.106237</td>\n      <td>0.255863</td>\n      <td>-0.195050</td>\n      <td>0.175015</td>\n      <td>...</td>\n      <td>2.118806</td>\n      <td>1.950530</td>\n      <td>2.485330</td>\n      <td>1.140412</td>\n      <td>2.213701</td>\n      <td>2.169538</td>\n      <td>2.587536</td>\n      <td>0.745009</td>\n      <td>0.749574</td>\n      <td>0.839705</td>\n    </tr>\n    <tr>\n      <th>AH1</th>\n      <th>41.5</th>\n      <td>-0.323308</td>\n      <td>-0.037860</td>\n      <td>-0.711544</td>\n      <td>-0.305593</td>\n      <td>-0.227577</td>\n      <td>0.013477</td>\n      <td>-0.423758</td>\n      <td>-0.185166</td>\n      <td>-0.366852</td>\n      <td>-0.494792</td>\n      <td>...</td>\n      <td>1.469235</td>\n      <td>1.521187</td>\n      <td>1.966960</td>\n      <td>0.732602</td>\n      <td>1.522058</td>\n      <td>1.714008</td>\n      <td>1.818767</td>\n      <td>0.547487</td>\n      <td>0.553544</td>\n      <td>0.593992</td>\n    </tr>\n    <tr>\n      <th>AM1</th>\n      <th>27.0</th>\n      <td>0.175577</td>\n      <td>0.412751</td>\n      <td>0.288554</td>\n      <td>0.048740</td>\n      <td>0.684970</td>\n      <td>0.351564</td>\n      <td>0.214291</td>\n      <td>0.090384</td>\n      <td>0.558743</td>\n      <td>-0.080070</td>\n      <td>...</td>\n      <td>1.986381</td>\n      <td>2.447330</td>\n      <td>2.141479</td>\n      <td>1.207785</td>\n      <td>2.172474</td>\n      <td>1.945818</td>\n      <td>2.561212</td>\n      <td>0.683115</td>\n      <td>0.652229</td>\n      <td>0.697552</td>\n    </tr>\n    <tr>\n      <th>AP1</th>\n      <th>17.0</th>\n      <td>0.467508</td>\n      <td>0.169592</td>\n      <td>-0.193752</td>\n      <td>-0.240978</td>\n      <td>0.309113</td>\n      <td>-0.226975</td>\n      <td>0.522462</td>\n      <td>-0.106493</td>\n      <td>-0.040279</td>\n      <td>-0.234033</td>\n      <td>...</td>\n      <td>1.900803</td>\n      <td>1.932110</td>\n      <td>2.251900</td>\n      <td>1.140876</td>\n      <td>1.687866</td>\n      <td>1.951376</td>\n      <td>2.173467</td>\n      <td>0.897872</td>\n      <td>0.850509</td>\n      <td>0.879986</td>\n    </tr>\n    <tr>\n      <th>AV1</th>\n      <th>48.5</th>\n      <td>-0.038907</td>\n      <td>-0.278500</td>\n      <td>0.676340</td>\n      <td>-0.182464</td>\n      <td>0.137662</td>\n      <td>0.267408</td>\n      <td>-0.640726</td>\n      <td>-0.031024</td>\n      <td>-0.274715</td>\n      <td>0.355416</td>\n      <td>...</td>\n      <td>1.825501</td>\n      <td>1.970395</td>\n      <td>2.175813</td>\n      <td>1.320186</td>\n      <td>1.759614</td>\n      <td>1.811613</td>\n      <td>2.122644</td>\n      <td>0.793224</td>\n      <td>0.761789</td>\n      <td>0.989130</td>\n    </tr>\n    <tr>\n      <th>BN1</th>\n      <th>75.0</th>\n      <td>0.056349</td>\n      <td>-0.016979</td>\n      <td>-0.353304</td>\n      <td>-0.461987</td>\n      <td>-0.095968</td>\n      <td>-0.302246</td>\n      <td>-0.727874</td>\n      <td>-0.240385</td>\n      <td>-0.361364</td>\n      <td>-0.243420</td>\n      <td>...</td>\n      <td>1.828686</td>\n      <td>2.417146</td>\n      <td>2.015882</td>\n      <td>1.446961</td>\n      <td>2.021826</td>\n      <td>1.898347</td>\n      <td>3.550143</td>\n      <td>1.100902</td>\n      <td>1.116964</td>\n      <td>1.013991</td>\n    </tr>\n    <tr>\n      <th>CD1</th>\n      <th>15.0</th>\n      <td>0.051840</td>\n      <td>-0.055279</td>\n      <td>-0.196941</td>\n      <td>-0.095444</td>\n      <td>-0.239501</td>\n      <td>-0.159295</td>\n      <td>0.439867</td>\n      <td>0.079581</td>\n      <td>-0.485805</td>\n      <td>-0.051421</td>\n      <td>...</td>\n      <td>1.917480</td>\n      <td>1.779552</td>\n      <td>2.332097</td>\n      <td>1.320598</td>\n      <td>1.933629</td>\n      <td>2.319423</td>\n      <td>2.608868</td>\n      <td>0.798232</td>\n      <td>0.860821</td>\n      <td>1.144511</td>\n    </tr>\n    <tr>\n      <th>CG1</th>\n      <th>32.0</th>\n      <td>0.108409</td>\n      <td>0.024720</td>\n      <td>0.175183</td>\n      <td>-0.050737</td>\n      <td>-0.377077</td>\n      <td>-0.248930</td>\n      <td>-0.497208</td>\n      <td>0.301012</td>\n      <td>-0.195419</td>\n      <td>0.396552</td>\n      <td>...</td>\n      <td>1.843634</td>\n      <td>1.816909</td>\n      <td>2.206035</td>\n      <td>1.227193</td>\n      <td>2.005296</td>\n      <td>1.990659</td>\n      <td>2.628626</td>\n      <td>0.945395</td>\n      <td>0.928960</td>\n      <td>0.836702</td>\n    </tr>\n    <tr>\n      <th>CH1</th>\n      <th>39.0</th>\n      <td>0.177342</td>\n      <td>-0.087421</td>\n      <td>-0.016997</td>\n      <td>-0.433329</td>\n      <td>-0.306632</td>\n      <td>-0.416012</td>\n      <td>-0.262217</td>\n      <td>0.006448</td>\n      <td>-0.122991</td>\n      <td>-0.126323</td>\n      <td>...</td>\n      <td>1.691757</td>\n      <td>1.820045</td>\n      <td>2.101218</td>\n      <td>1.017975</td>\n      <td>1.840660</td>\n      <td>1.822882</td>\n      <td>1.800403</td>\n      <td>0.740360</td>\n      <td>0.711458</td>\n      <td>0.766755</td>\n    </tr>\n    <tr>\n      <th>CK1</th>\n      <th>34.0</th>\n      <td>0.086924</td>\n      <td>0.137110</td>\n      <td>0.299040</td>\n      <td>-0.123841</td>\n      <td>0.162907</td>\n      <td>0.514591</td>\n      <td>0.492125</td>\n      <td>0.150974</td>\n      <td>0.323942</td>\n      <td>0.202864</td>\n      <td>...</td>\n      <td>1.802336</td>\n      <td>1.631477</td>\n      <td>2.053382</td>\n      <td>0.788506</td>\n      <td>1.592865</td>\n      <td>1.981506</td>\n      <td>2.190200</td>\n      <td>0.624014</td>\n      <td>0.559237</td>\n      <td>0.503816</td>\n    </tr>\n    <tr>\n      <th>CM1</th>\n      <th>29.5</th>\n      <td>0.210084</td>\n      <td>-0.287603</td>\n      <td>0.282841</td>\n      <td>0.191320</td>\n      <td>-0.181049</td>\n      <td>0.408398</td>\n      <td>-0.415814</td>\n      <td>0.109322</td>\n      <td>-0.269060</td>\n      <td>-0.058011</td>\n      <td>...</td>\n      <td>1.714557</td>\n      <td>1.964727</td>\n      <td>1.917768</td>\n      <td>1.043854</td>\n      <td>1.779677</td>\n      <td>1.965310</td>\n      <td>2.384994</td>\n      <td>0.639517</td>\n      <td>0.778450</td>\n      <td>0.915022</td>\n    </tr>\n    <tr>\n      <th>CM2</th>\n      <th>73.0</th>\n      <td>0.268694</td>\n      <td>0.294601</td>\n      <td>-0.253968</td>\n      <td>0.040571</td>\n      <td>-0.332977</td>\n      <td>-0.143175</td>\n      <td>-0.212977</td>\n      <td>0.415070</td>\n      <td>0.127135</td>\n      <td>0.055575</td>\n      <td>...</td>\n      <td>2.143757</td>\n      <td>1.788549</td>\n      <td>2.425416</td>\n      <td>1.421685</td>\n      <td>1.916807</td>\n      <td>1.834765</td>\n      <td>2.582323</td>\n      <td>0.663826</td>\n      <td>0.742096</td>\n      <td>0.416214</td>\n    </tr>\n    <tr>\n      <th>EH1</th>\n      <th>21.0</th>\n      <td>0.387339</td>\n      <td>0.257275</td>\n      <td>0.291328</td>\n      <td>0.031696</td>\n      <td>0.448034</td>\n      <td>0.225895</td>\n      <td>-0.088108</td>\n      <td>0.405360</td>\n      <td>0.145292</td>\n      <td>0.361376</td>\n      <td>...</td>\n      <td>2.139039</td>\n      <td>2.438279</td>\n      <td>2.438391</td>\n      <td>1.240941</td>\n      <td>2.022860</td>\n      <td>1.749947</td>\n      <td>2.857961</td>\n      <td>0.838048</td>\n      <td>0.705073</td>\n      <td>1.037394</td>\n    </tr>\n    <tr>\n      <th>EK1</th>\n      <th>12.0</th>\n      <td>0.013718</td>\n      <td>-0.145739</td>\n      <td>0.582988</td>\n      <td>0.007816</td>\n      <td>0.292638</td>\n      <td>0.670939</td>\n      <td>0.623340</td>\n      <td>-0.264783</td>\n      <td>0.040501</td>\n      <td>0.387089</td>\n      <td>...</td>\n      <td>1.998436</td>\n      <td>1.958971</td>\n      <td>2.122259</td>\n      <td>1.363259</td>\n      <td>2.042452</td>\n      <td>1.750775</td>\n      <td>2.079249</td>\n      <td>1.069446</td>\n      <td>0.997267</td>\n      <td>0.930993</td>\n    </tr>\n    <tr>\n      <th>HK1</th>\n      <th>17.0</th>\n      <td>0.321216</td>\n      <td>0.582425</td>\n      <td>0.415549</td>\n      <td>0.005094</td>\n      <td>0.273420</td>\n      <td>0.180396</td>\n      <td>0.045845</td>\n      <td>0.132566</td>\n      <td>0.025067</td>\n      <td>0.036989</td>\n      <td>...</td>\n      <td>2.017204</td>\n      <td>1.528434</td>\n      <td>2.036788</td>\n      <td>0.782307</td>\n      <td>1.652856</td>\n      <td>1.783014</td>\n      <td>1.970913</td>\n      <td>0.655538</td>\n      <td>0.538979</td>\n      <td>0.522497</td>\n    </tr>\n    <tr>\n      <th>HS1</th>\n      <th>67.0</th>\n      <td>0.170226</td>\n      <td>-0.168491</td>\n      <td>0.353160</td>\n      <td>-0.060915</td>\n      <td>0.104842</td>\n      <td>0.046738</td>\n      <td>0.540557</td>\n      <td>0.363904</td>\n      <td>-0.218475</td>\n      <td>0.434677</td>\n      <td>...</td>\n      <td>1.749179</td>\n      <td>1.640362</td>\n      <td>1.796140</td>\n      <td>0.959187</td>\n      <td>1.704678</td>\n      <td>2.418599</td>\n      <td>2.570555</td>\n      <td>0.651496</td>\n      <td>0.533009</td>\n      <td>0.629259</td>\n    </tr>\n    <tr>\n      <th>JC1</th>\n      <th>12.5</th>\n      <td>0.441986</td>\n      <td>0.234091</td>\n      <td>0.437221</td>\n      <td>-0.171470</td>\n      <td>0.216009</td>\n      <td>-0.057136</td>\n      <td>0.280812</td>\n      <td>-0.175299</td>\n      <td>0.204475</td>\n      <td>0.016970</td>\n      <td>...</td>\n      <td>2.017339</td>\n      <td>1.882053</td>\n      <td>2.062122</td>\n      <td>1.285489</td>\n      <td>1.537065</td>\n      <td>1.875158</td>\n      <td>2.390729</td>\n      <td>0.851868</td>\n      <td>0.802638</td>\n      <td>0.671588</td>\n    </tr>\n    <tr>\n      <th>JH1</th>\n      <th>8.5</th>\n      <td>-0.026666</td>\n      <td>0.106128</td>\n      <td>0.104092</td>\n      <td>0.458677</td>\n      <td>-0.207485</td>\n      <td>-0.334236</td>\n      <td>0.130646</td>\n      <td>-0.122474</td>\n      <td>0.023361</td>\n      <td>0.134300</td>\n      <td>...</td>\n      <td>1.998840</td>\n      <td>2.415192</td>\n      <td>2.472577</td>\n      <td>1.306469</td>\n      <td>2.494458</td>\n      <td>2.319943</td>\n      <td>2.074895</td>\n      <td>1.041253</td>\n      <td>1.024920</td>\n      <td>0.842064</td>\n    </tr>\n    <tr>\n      <th>JM1</th>\n      <th>34.0</th>\n      <td>-0.281974</td>\n      <td>-0.076701</td>\n      <td>0.006779</td>\n      <td>-0.074407</td>\n      <td>0.129751</td>\n      <td>0.341420</td>\n      <td>-0.310428</td>\n      <td>-0.134713</td>\n      <td>-0.202092</td>\n      <td>0.027413</td>\n      <td>...</td>\n      <td>2.478670</td>\n      <td>2.192126</td>\n      <td>2.661035</td>\n      <td>1.260877</td>\n      <td>1.729108</td>\n      <td>1.861761</td>\n      <td>1.587026</td>\n      <td>1.256556</td>\n      <td>1.091728</td>\n      <td>0.995402</td>\n    </tr>\n    <tr>\n      <th>JR1</th>\n      <th>34.0</th>\n      <td>-0.072512</td>\n      <td>0.251876</td>\n      <td>-0.125334</td>\n      <td>0.136915</td>\n      <td>-0.049604</td>\n      <td>-0.354429</td>\n      <td>-0.095955</td>\n      <td>0.192945</td>\n      <td>-0.159106</td>\n      <td>-0.083395</td>\n      <td>...</td>\n      <td>1.767363</td>\n      <td>1.844408</td>\n      <td>2.340844</td>\n      <td>1.113321</td>\n      <td>1.676130</td>\n      <td>1.741595</td>\n      <td>2.407909</td>\n      <td>0.932248</td>\n      <td>0.843510</td>\n      <td>0.939630</td>\n    </tr>\n    <tr>\n      <th>JS1</th>\n      <th>57.0</th>\n      <td>0.057821</td>\n      <td>0.119935</td>\n      <td>0.076817</td>\n      <td>-0.161564</td>\n      <td>0.178161</td>\n      <td>0.385102</td>\n      <td>-0.150975</td>\n      <td>-0.237924</td>\n      <td>-0.045934</td>\n      <td>-0.095143</td>\n      <td>...</td>\n      <td>2.235044</td>\n      <td>2.277080</td>\n      <td>2.747927</td>\n      <td>1.405836</td>\n      <td>2.198853</td>\n      <td>3.195862</td>\n      <td>2.860114</td>\n      <td>0.852203</td>\n      <td>0.809016</td>\n      <td>0.767970</td>\n    </tr>\n    <tr>\n      <th>KK1</th>\n      <th>11.5</th>\n      <td>-0.025902</td>\n      <td>0.484708</td>\n      <td>0.069643</td>\n      <td>-0.103952</td>\n      <td>0.453943</td>\n      <td>0.125557</td>\n      <td>0.744675</td>\n      <td>0.217756</td>\n      <td>0.099546</td>\n      <td>0.239251</td>\n      <td>...</td>\n      <td>1.844331</td>\n      <td>1.939742</td>\n      <td>2.342958</td>\n      <td>1.196451</td>\n      <td>1.699696</td>\n      <td>2.183249</td>\n      <td>1.957275</td>\n      <td>0.683781</td>\n      <td>0.654229</td>\n      <td>0.587645</td>\n    </tr>\n    <tr>\n      <th>KP1</th>\n      <th>38.0</th>\n      <td>0.131821</td>\n      <td>0.172627</td>\n      <td>0.242722</td>\n      <td>-0.000527</td>\n      <td>0.143837</td>\n      <td>-0.173275</td>\n      <td>0.350981</td>\n      <td>0.013444</td>\n      <td>-0.083172</td>\n      <td>0.126382</td>\n      <td>...</td>\n      <td>1.736222</td>\n      <td>1.624049</td>\n      <td>1.833292</td>\n      <td>0.967197</td>\n      <td>1.754282</td>\n      <td>1.623096</td>\n      <td>2.350881</td>\n      <td>0.771450</td>\n      <td>0.716038</td>\n      <td>0.483066</td>\n    </tr>\n    <tr>\n      <th>KR1</th>\n      <th>25.0</th>\n      <td>0.492554</td>\n      <td>0.689035</td>\n      <td>0.391546</td>\n      <td>0.061062</td>\n      <td>0.117760</td>\n      <td>0.363120</td>\n      <td>0.007332</td>\n      <td>0.464407</td>\n      <td>0.486510</td>\n      <td>0.257641</td>\n      <td>...</td>\n      <td>2.016530</td>\n      <td>1.970034</td>\n      <td>2.122277</td>\n      <td>1.061288</td>\n      <td>1.834982</td>\n      <td>2.129779</td>\n      <td>2.158371</td>\n      <td>0.621210</td>\n      <td>0.747211</td>\n      <td>0.775904</td>\n    </tr>\n    <tr>\n      <th>LB1</th>\n      <th>12.0</th>\n      <td>-0.088604</td>\n      <td>0.422639</td>\n      <td>0.162973</td>\n      <td>-0.061952</td>\n      <td>0.070794</td>\n      <td>-0.100209</td>\n      <td>0.464483</td>\n      <td>0.166278</td>\n      <td>-0.112810</td>\n      <td>0.365456</td>\n      <td>...</td>\n      <td>1.764201</td>\n      <td>1.803203</td>\n      <td>1.943513</td>\n      <td>1.059673</td>\n      <td>1.532155</td>\n      <td>1.502479</td>\n      <td>2.226467</td>\n      <td>0.758493</td>\n      <td>0.740046</td>\n      <td>0.639216</td>\n    </tr>\n    <tr>\n      <th>MB1</th>\n      <th>39.0</th>\n      <td>-0.056847</td>\n      <td>0.236683</td>\n      <td>-0.310333</td>\n      <td>-0.420323</td>\n      <td>0.395842</td>\n      <td>-0.279680</td>\n      <td>-0.298043</td>\n      <td>-0.335835</td>\n      <td>-0.135304</td>\n      <td>-0.234160</td>\n      <td>...</td>\n      <td>1.809708</td>\n      <td>1.487029</td>\n      <td>1.875054</td>\n      <td>0.960740</td>\n      <td>1.361371</td>\n      <td>1.568102</td>\n      <td>2.218430</td>\n      <td>0.751340</td>\n      <td>0.673188</td>\n      <td>0.556428</td>\n    </tr>\n    <tr>\n      <th>MG1</th>\n      <th>57.0</th>\n      <td>-0.252071</td>\n      <td>-0.571489</td>\n      <td>-0.340796</td>\n      <td>-0.205559</td>\n      <td>-0.285394</td>\n      <td>-0.190710</td>\n      <td>-0.268854</td>\n      <td>-0.198966</td>\n      <td>-0.395889</td>\n      <td>-0.384115</td>\n      <td>...</td>\n      <td>1.588440</td>\n      <td>1.480362</td>\n      <td>1.638124</td>\n      <td>0.962248</td>\n      <td>1.373220</td>\n      <td>1.690456</td>\n      <td>1.676091</td>\n      <td>0.580182</td>\n      <td>0.526279</td>\n      <td>0.386052</td>\n    </tr>\n    <tr>\n      <th>MP1</th>\n      <th>45.0</th>\n      <td>0.038101</td>\n      <td>-0.436873</td>\n      <td>-0.311200</td>\n      <td>-0.566607</td>\n      <td>-0.674823</td>\n      <td>-0.209500</td>\n      <td>-1.388130</td>\n      <td>-0.376981</td>\n      <td>-0.475804</td>\n      <td>-0.486909</td>\n      <td>...</td>\n      <td>2.122926</td>\n      <td>2.598077</td>\n      <td>2.372456</td>\n      <td>1.378973</td>\n      <td>2.369778</td>\n      <td>2.705829</td>\n      <td>2.800093</td>\n      <td>0.856064</td>\n      <td>0.862693</td>\n      <td>0.950355</td>\n    </tr>\n    <tr>\n      <th>NW1</th>\n      <th>26.0</th>\n      <td>-0.284932</td>\n      <td>-0.155696</td>\n      <td>0.231738</td>\n      <td>-0.321735</td>\n      <td>0.158029</td>\n      <td>0.738793</td>\n      <td>-0.509783</td>\n      <td>-0.087884</td>\n      <td>-0.138356</td>\n      <td>0.496930</td>\n      <td>...</td>\n      <td>3.079307</td>\n      <td>2.663945</td>\n      <td>2.993097</td>\n      <td>1.766643</td>\n      <td>2.633523</td>\n      <td>3.045875</td>\n      <td>2.619249</td>\n      <td>0.956131</td>\n      <td>0.881309</td>\n      <td>0.802078</td>\n    </tr>\n    <tr>\n      <th>SK1</th>\n      <th>39.0</th>\n      <td>0.353440</td>\n      <td>0.344001</td>\n      <td>-0.212267</td>\n      <td>0.011286</td>\n      <td>-0.277125</td>\n      <td>-0.489517</td>\n      <td>-0.061978</td>\n      <td>0.330996</td>\n      <td>0.195117</td>\n      <td>-0.078838</td>\n      <td>...</td>\n      <td>2.246061</td>\n      <td>2.085018</td>\n      <td>2.281964</td>\n      <td>1.093341</td>\n      <td>1.855895</td>\n      <td>2.148046</td>\n      <td>2.295968</td>\n      <td>0.747645</td>\n      <td>0.724677</td>\n      <td>0.638925</td>\n    </tr>\n    <tr>\n      <th>SM2</th>\n      <th>44.0</th>\n      <td>0.156058</td>\n      <td>0.257919</td>\n      <td>0.384763</td>\n      <td>0.080896</td>\n      <td>0.152030</td>\n      <td>0.202089</td>\n      <td>0.151459</td>\n      <td>-0.094142</td>\n      <td>-0.164098</td>\n      <td>0.193566</td>\n      <td>...</td>\n      <td>2.368565</td>\n      <td>2.319486</td>\n      <td>3.058632</td>\n      <td>1.231436</td>\n      <td>2.313356</td>\n      <td>2.359852</td>\n      <td>2.787416</td>\n      <td>0.844175</td>\n      <td>0.771756</td>\n      <td>0.709416</td>\n    </tr>\n    <tr>\n      <th>SR1</th>\n      <th>21.5</th>\n      <td>-0.047300</td>\n      <td>-0.070251</td>\n      <td>-0.042661</td>\n      <td>-0.290926</td>\n      <td>-0.490961</td>\n      <td>0.141022</td>\n      <td>-0.125938</td>\n      <td>-0.084477</td>\n      <td>-0.511392</td>\n      <td>0.142195</td>\n      <td>...</td>\n      <td>1.680886</td>\n      <td>1.754788</td>\n      <td>1.948046</td>\n      <td>0.829129</td>\n      <td>1.603257</td>\n      <td>2.127442</td>\n      <td>2.237622</td>\n      <td>0.505732</td>\n      <td>0.526616</td>\n      <td>0.540050</td>\n    </tr>\n    <tr>\n      <th>UP1</th>\n      <th>46.0</th>\n      <td>0.141589</td>\n      <td>0.151221</td>\n      <td>0.142272</td>\n      <td>0.022815</td>\n      <td>0.133355</td>\n      <td>0.163306</td>\n      <td>1.118926</td>\n      <td>-0.426311</td>\n      <td>-0.036800</td>\n      <td>0.057457</td>\n      <td>...</td>\n      <td>1.941150</td>\n      <td>2.477219</td>\n      <td>2.345197</td>\n      <td>1.137999</td>\n      <td>1.845406</td>\n      <td>2.196156</td>\n      <td>2.911876</td>\n      <td>0.910662</td>\n      <td>0.953792</td>\n      <td>0.818327</td>\n    </tr>\n    <tr>\n      <th>WK1</th>\n      <th>62.0</th>\n      <td>0.068048</td>\n      <td>0.233606</td>\n      <td>0.053965</td>\n      <td>0.047418</td>\n      <td>-0.042987</td>\n      <td>0.142878</td>\n      <td>-0.112929</td>\n      <td>0.186223</td>\n      <td>0.062975</td>\n      <td>-0.040481</td>\n      <td>...</td>\n      <td>1.727043</td>\n      <td>1.453272</td>\n      <td>1.633706</td>\n      <td>1.006814</td>\n      <td>1.713639</td>\n      <td>1.724839</td>\n      <td>1.926901</td>\n      <td>0.593836</td>\n      <td>0.605323</td>\n      <td>0.420779</td>\n    </tr>\n    <tr>\n      <th>ZN1</th>\n      <th>55.0</th>\n      <td>0.146824</td>\n      <td>-0.541973</td>\n      <td>-0.176711</td>\n      <td>-0.314063</td>\n      <td>0.160933</td>\n      <td>-0.103045</td>\n      <td>-0.715399</td>\n      <td>-0.297575</td>\n      <td>-0.164273</td>\n      <td>-0.387589</td>\n      <td>...</td>\n      <td>2.155943</td>\n      <td>1.805936</td>\n      <td>1.947398</td>\n      <td>1.077923</td>\n      <td>1.944697</td>\n      <td>2.245211</td>\n      <td>2.400097</td>\n      <td>0.873998</td>\n      <td>0.920408</td>\n      <td>0.869525</td>\n    </tr>\n  </tbody>\n</table>\n<p>36 rows Ã— 85 columns</p>\n</div>"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfecc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2q/y6tqndfj1yqfc7hcdp99tz_00000gn/T/ipykernel_12408/1566465353.py:1: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  X = dfecc.reset_index().drop(['Subject', 'Score_median_first20'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "X = dfecc.reset_index().drop(['Subject', 'Score_median_first20'], axis=1)\n",
    "\n",
    "y = dfecc.reset_index()['Score_median_first20']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "                          value                           \\\nepoch_                      b2e                            \nregion 7Networks_LH_Cont_Cing_1 7Networks_LH_Cont_Cing_2   \n0                      0.703477                -0.927863   \n1                      0.357591                -0.575116   \n2                     -0.014645                -0.007817   \n3                      0.363827                -0.455096   \n4                      1.243632                 0.095185   \n5                     -1.008693                 1.160782   \n6                     -0.177668                -0.631989   \n7                     -0.338410                -0.190189   \n8                      0.026779                 0.418469   \n9                      0.711932                 1.881008   \n10                     0.206989                 0.368045   \n11                     0.790264                -0.490761   \n12                     0.014010                 0.586256   \n13                     1.182094                 0.056378   \n14                    -0.101774                -0.528102   \n15                    -0.265762                 0.420327   \n16                     0.059892                 0.633167   \n17                    -0.626487                -0.158431   \n18                    -0.866298                -0.098429   \n19                    -0.218577                -0.386367   \n20                     0.072564                 0.199435   \n21                     0.187944                 0.021905   \n22                     0.802707                 0.666476   \n23                     1.286294                 0.139949   \n24                    -0.264532                -0.343811   \n25                    -0.117901                 0.176652   \n26                     1.606390                -0.250953   \n27                     0.402983                -0.098029   \n28                     0.129998                -1.530500   \n29                    -0.627209                -0.328759   \n30                     0.701957                 0.473165   \n31                     0.692927                -0.046108   \n32                    -0.914123                -0.900474   \n33                    -0.126119                 1.328713   \n34                    -0.568768                -0.244131   \n35                     0.675976                 0.855875   \n\n                                                          \\\nepoch_                                                     \nregion 7Networks_LH_Cont_Cing_3 7Networks_LH_Cont_Cing_4   \n0                      0.166523                -0.016294   \n1                     -0.198989                -0.607698   \n2                     -0.355955                -1.415309   \n3                     -0.745145                 0.080842   \n4                      1.172546                -1.218897   \n5                     -0.221900                 0.478659   \n6                      0.354508                -0.721208   \n7                      0.074796                -0.044014   \n8                      0.012713                 0.640000   \n9                     -0.205136                 0.068042   \n10                     0.451437                -0.305220   \n11                    -0.296272                -1.581116   \n12                    -0.973766                 0.215011   \n13                    -0.676597                 0.491842   \n14                    -0.076403                 0.872357   \n15                     0.454712                 0.302573   \n16                     0.761280                 0.760854   \n17                    -0.676748                -0.803616   \n18                    -0.573917                 0.431163   \n19                    -0.161368                -0.327313   \n20                     0.147559                -0.179394   \n21                     0.226958                 0.127067   \n22                    -0.064805                 0.980947   \n23                    -0.433095                -0.043339   \n24                    -0.577846                -0.340454   \n25                     0.410014                 0.427881   \n26                    -0.423623                 0.862232   \n27                    -0.648626                -0.847819   \n28                     0.425615                 0.356731   \n29                     1.466718                 0.206775   \n30                     0.395478                 0.401670   \n31                    -0.993510                 0.325574   \n32                    -0.150320                -0.404613   \n33                     0.159579                 0.417866   \n34                     0.627234                -0.048669   \n35                     0.293531                 0.825723   \n\n                                                          \\\nepoch_                                                     \nregion 7Networks_LH_Cont_Cing_5 7Networks_LH_Cont_Cing_6   \n0                      0.476628                -0.004921   \n1                      0.018521                 0.420202   \n2                     -0.647757                 0.322096   \n3                      0.147343                 1.912837   \n4                     -0.205928                -0.123442   \n5                      0.391829                 0.344078   \n6                      0.350087                 1.169053   \n7                     -0.744120                -0.078219   \n8                      0.035729                -0.423393   \n9                     -0.645405                 0.469290   \n10                     0.104208                 0.130911   \n11                     0.239142                -0.217248   \n12                    -0.220170                -0.132289   \n13                    -0.121874                 0.149285   \n14                     0.179179                -0.259624   \n15                     1.493910                -0.314175   \n16                    -0.119647                -0.438942   \n17                     0.167069                -0.676502   \n18                     1.353579                 1.072432   \n19                     0.601797                -0.326254   \n20                    -0.047295                -0.479883   \n21                    -0.443422                -0.989702   \n22                    -0.188047                -0.385459   \n23                     1.264618                -0.467620   \n24                     0.038294                 0.636407   \n25                     2.424198                 1.109578   \n26                     1.236941                -0.555238   \n27                    -0.165234                -0.340926   \n28                     1.510383                 0.298601   \n29                     0.283206                 0.259588   \n30                     1.266172                 0.462852   \n31                    -0.144758                 0.749779   \n32                    -0.275390                -0.715905   \n33                    -0.222756                -1.191773   \n34                    -0.993898                 0.306194   \n35                     1.368092                -0.286602   \n\n                                                          \\\nepoch_                                                     \nregion 7Networks_LH_Cont_Cing_7 7Networks_LH_Cont_Cing_8   \n0                     -0.108299                 1.454048   \n1                     -0.551192                 0.286538   \n2                      0.242143                -0.435097   \n3                     -0.240416                 0.628372   \n4                     -0.418317                 2.054777   \n5                     -1.826029                -0.152947   \n6                      0.136857                 0.177055   \n7                      0.389375                 0.363241   \n8                     -0.401305                 0.066129   \n9                     -0.173488                 1.053547   \n10                    -0.826910                 1.397730   \n11                    -0.480448                 0.383090   \n12                     0.135526                 1.820222   \n13                     0.814536                 0.959054   \n14                     0.074282                -0.002134   \n15                     1.389485                 1.107427   \n16                     0.747495                -0.197692   \n17                     0.816456                -0.367254   \n18                    -0.322764                 0.234283   \n19                    -2.164355                 0.071194   \n20                     0.235401                 0.677919   \n21                     0.835364                 0.506010   \n22                    -0.516367                 0.324996   \n23                    -0.611925                 0.102487   \n24                     0.109409                 2.787275   \n25                     0.262734                -0.513797   \n26                    -0.240114                 2.005884   \n27                    -0.610192                -1.022690   \n28                     1.325150                 0.451775   \n29                    -0.482470                 0.839048   \n30                    -0.004709                 0.551558   \n31                    -1.091492                -0.905298   \n32                     0.048271                -0.013360   \n33                     0.730492                 1.378051   \n34                    -0.479014                -0.298853   \n35                    -0.560994                 0.598876   \n\n                                                         ...             \\\nepoch_                                                   ...       late   \nregion 7Networks_LH_Cont_OFC_1 7Networks_LH_Cont_PFCd_1  ... Right VIIb   \n0                    -1.324677                -1.434679  ...   0.735084   \n1                     0.287896                -1.012053  ...   0.326792   \n2                    -0.326071                 0.195982  ...   0.658580   \n3                     0.333513                 0.254071  ...   0.569318   \n4                    -0.128411                 0.441954  ...   0.950575   \n5                    -0.212664                -0.319781  ...   0.340569   \n6                    -0.758720                 1.498564  ...   1.154001   \n7                     0.718493                 0.178702  ...   0.611949   \n8                     0.225000                 1.053589  ...   0.727498   \n9                    -0.403116                -0.452387  ...   0.698473   \n10                    0.347226                -1.797023  ...   0.471513   \n11                    0.012946                -1.059767  ...   0.907239   \n12                    0.051257                -0.678915  ...   0.647399   \n13                    0.091563                 0.072141  ...   0.557015   \n14                   -0.386564                -0.082914  ...   0.994882   \n15                    0.147905                -0.614600  ...   0.465867   \n16                    0.081559                -0.151836  ...   0.351277   \n17                   -0.419163                -0.968038  ...   0.766764   \n18                    1.271205                 1.618540  ...   0.789729   \n19                   -0.438110                -0.937343  ...   1.069480   \n20                    0.256666                 1.348302  ...   0.549596   \n21                    0.050489                -0.110617  ...   0.626207   \n22                    0.255761                -0.142576  ...   0.525160   \n23                    0.166288                 0.618344  ...   0.449427   \n24                    0.531051                 0.593885  ...   0.561514   \n25                    0.417501                -2.378573  ...   0.680568   \n26                    0.229817                -1.170817  ...   0.832733   \n27                   -0.258544                 0.137870  ...   0.398340   \n28                    0.876342                 0.154158  ...   0.771897   \n29                   -0.836016                 0.771969  ...   0.248630   \n30                   -0.806474                 0.603992  ...   0.525116   \n31                   -0.361705                 0.367910  ...   0.953161   \n32                   -0.295373                -0.543669  ...   0.897693   \n33                    0.205499                 1.236511  ...   0.740850   \n34                    0.047103                 0.978920  ...   0.484060   \n35                    0.676910                -1.282450  ...   0.849813   \n\n                                                                   \\\nepoch_                                                              \nregion   Right X Vermis Crus I Vermis Crus II Vermis IX Vermis VI   \n0       0.855647      0.517545       0.588341  0.720502  0.841823   \n1       0.857592      0.721927       0.615470  0.656858  1.279152   \n2       0.448548      0.617202       0.907647  0.634930  0.671958   \n3       0.825648      0.433107       0.619285  0.668363  1.065356   \n4       0.420350      0.642552       1.322419  0.893142  1.357487   \n5       0.845481      0.892774       0.557665  1.142608  1.274956   \n6       1.229276      1.045766       0.952320  1.189342  0.965396   \n7       0.398364      0.841575       1.803682  0.905942  1.759225   \n8       0.601820      0.907855       0.824140  0.741622  0.793998   \n9       0.579019      1.058151       0.763656  0.659289  0.904778   \n10      0.371195      0.529332       0.398287  0.719298  0.819254   \n11      0.521250      0.568995       0.818254  0.949733  1.753522   \n12      0.786828      0.262157       0.430094  0.456120  0.538695   \n13      0.277199      0.985541       1.229716  0.919721  1.383838   \n14      0.974937      1.347045       1.076800  0.932643  0.779570   \n15      0.450466      0.356457       0.198023  0.461339  0.828802   \n16      0.714060      0.634774       0.634865  0.673263  0.619485   \n17      0.627427      0.562448       0.635870  0.519491  0.530216   \n18      0.781411      0.885491       0.940210  0.734011  0.830063   \n19      1.049201      1.067519       1.421555  0.763547  0.841344   \n20      0.691904      0.959037       1.003023  0.896519  1.241046   \n21      0.593061      0.797011       0.499654  0.919041  1.026195   \n22      0.543367      0.769446       0.729830  0.529320  0.599139   \n23      0.437719      0.555973       0.478075  0.380226  0.647384   \n24      0.710776      0.243444       0.458907  0.652025  1.397664   \n25      0.784288      0.687458       0.635623  0.598925  0.450300   \n26      0.332151      0.673872       0.470825  0.405546  0.681677   \n27      0.664014      0.490569       0.152743  0.421731  0.537940   \n28      0.850772      0.774313       0.779317  0.723018  1.243650   \n29      0.852799      1.054467       1.073491  0.469751  0.735968   \n30      0.275932      0.600906       0.384481  0.746002  0.692177   \n31      0.533213      0.674875       0.743851  0.706055  0.795700   \n32      0.324590      0.519305       0.420123  0.485783  0.707940   \n33      0.955943      0.481114       0.824568  0.540927  1.745259   \n34      0.488738      0.304058       0.366015  0.724859  0.519312   \n35      0.750543      0.704989       0.655117  0.790406  1.528952   \n\n                                                        \nepoch_                                                  \nregion Vermis VIIIa Vermis VIIIb Vermis VIIb  Vermis X  \n0          1.186272     1.111554    0.727469  1.070183  \n1          1.529414     0.643396    0.717045  0.554377  \n2          0.318305     0.385403    0.429851  0.786640  \n3          0.437896     0.595521    0.596480  1.164412  \n4          0.910520     0.553791    0.623917  0.736064  \n5          0.750241     1.639472    0.778551  0.876777  \n6          1.080109     1.117570    0.922300  0.839124  \n7          0.750258     0.963737    1.569297  0.562372  \n8          0.793528     0.980815    0.906199  0.745458  \n9          0.691719     0.811686    0.641338  0.603422  \n10         0.126255     0.358638    0.615312  0.464150  \n11         0.916861     0.819690    1.009051  0.484069  \n12         0.233223     0.213230    0.598305  0.597890  \n13         1.215840     0.708408    1.272766  0.583322  \n14         0.795571     0.913330    0.622058  0.980927  \n15         0.574692     0.568767    0.587944  0.603948  \n16         0.554539     0.691272    0.550282  0.675595  \n17         0.687517     0.715920    0.882023  0.839218  \n18         0.840348     0.702657    0.920904  0.882827  \n19         1.080927     1.096643    0.901883  0.789802  \n20         0.791739     0.844695    1.036814  0.744169  \n21         0.504378     0.714164    0.648095  1.035227  \n22         0.571681     0.606157    0.419830  0.475757  \n23         0.269978     0.437252    0.531260  0.564383  \n24         0.752199     0.594811    0.605597  1.502583  \n25         0.664421     0.602573    0.977902  0.496523  \n26         0.675035     0.356435    0.609423  0.578609  \n27         0.318571     0.366131    0.379435  0.421296  \n28         1.365365     1.260640    0.811344  0.645193  \n29         0.664532     1.104602    0.308665  1.005152  \n30         0.750442     0.510012    0.722149  0.705228  \n31         0.719050     0.744596    0.625651  0.665553  \n32         0.412535     0.496743    0.406778  0.871189  \n33         0.975004     0.710905    0.785248  0.483595  \n34         0.411944     0.262892    0.341080  0.436073  \n35         1.148793     0.494938    0.828814  0.804193  \n\n[36 rows x 5200 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"21\" halign=\"left\">value</th>\n    </tr>\n    <tr>\n      <th>epoch_</th>\n      <th colspan=\"10\" halign=\"left\">b2e</th>\n      <th>...</th>\n      <th colspan=\"10\" halign=\"left\">late</th>\n    </tr>\n    <tr>\n      <th>region</th>\n      <th>7Networks_LH_Cont_Cing_1</th>\n      <th>7Networks_LH_Cont_Cing_2</th>\n      <th>7Networks_LH_Cont_Cing_3</th>\n      <th>7Networks_LH_Cont_Cing_4</th>\n      <th>7Networks_LH_Cont_Cing_5</th>\n      <th>7Networks_LH_Cont_Cing_6</th>\n      <th>7Networks_LH_Cont_Cing_7</th>\n      <th>7Networks_LH_Cont_Cing_8</th>\n      <th>7Networks_LH_Cont_OFC_1</th>\n      <th>7Networks_LH_Cont_PFCd_1</th>\n      <th>...</th>\n      <th>Right VIIb</th>\n      <th>Right X</th>\n      <th>Vermis Crus I</th>\n      <th>Vermis Crus II</th>\n      <th>Vermis IX</th>\n      <th>Vermis VI</th>\n      <th>Vermis VIIIa</th>\n      <th>Vermis VIIIb</th>\n      <th>Vermis VIIb</th>\n      <th>Vermis X</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.703477</td>\n      <td>-0.927863</td>\n      <td>0.166523</td>\n      <td>-0.016294</td>\n      <td>0.476628</td>\n      <td>-0.004921</td>\n      <td>-0.108299</td>\n      <td>1.454048</td>\n      <td>-1.324677</td>\n      <td>-1.434679</td>\n      <td>...</td>\n      <td>0.735084</td>\n      <td>0.855647</td>\n      <td>0.517545</td>\n      <td>0.588341</td>\n      <td>0.720502</td>\n      <td>0.841823</td>\n      <td>1.186272</td>\n      <td>1.111554</td>\n      <td>0.727469</td>\n      <td>1.070183</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.357591</td>\n      <td>-0.575116</td>\n      <td>-0.198989</td>\n      <td>-0.607698</td>\n      <td>0.018521</td>\n      <td>0.420202</td>\n      <td>-0.551192</td>\n      <td>0.286538</td>\n      <td>0.287896</td>\n      <td>-1.012053</td>\n      <td>...</td>\n      <td>0.326792</td>\n      <td>0.857592</td>\n      <td>0.721927</td>\n      <td>0.615470</td>\n      <td>0.656858</td>\n      <td>1.279152</td>\n      <td>1.529414</td>\n      <td>0.643396</td>\n      <td>0.717045</td>\n      <td>0.554377</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.014645</td>\n      <td>-0.007817</td>\n      <td>-0.355955</td>\n      <td>-1.415309</td>\n      <td>-0.647757</td>\n      <td>0.322096</td>\n      <td>0.242143</td>\n      <td>-0.435097</td>\n      <td>-0.326071</td>\n      <td>0.195982</td>\n      <td>...</td>\n      <td>0.658580</td>\n      <td>0.448548</td>\n      <td>0.617202</td>\n      <td>0.907647</td>\n      <td>0.634930</td>\n      <td>0.671958</td>\n      <td>0.318305</td>\n      <td>0.385403</td>\n      <td>0.429851</td>\n      <td>0.786640</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.363827</td>\n      <td>-0.455096</td>\n      <td>-0.745145</td>\n      <td>0.080842</td>\n      <td>0.147343</td>\n      <td>1.912837</td>\n      <td>-0.240416</td>\n      <td>0.628372</td>\n      <td>0.333513</td>\n      <td>0.254071</td>\n      <td>...</td>\n      <td>0.569318</td>\n      <td>0.825648</td>\n      <td>0.433107</td>\n      <td>0.619285</td>\n      <td>0.668363</td>\n      <td>1.065356</td>\n      <td>0.437896</td>\n      <td>0.595521</td>\n      <td>0.596480</td>\n      <td>1.164412</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.243632</td>\n      <td>0.095185</td>\n      <td>1.172546</td>\n      <td>-1.218897</td>\n      <td>-0.205928</td>\n      <td>-0.123442</td>\n      <td>-0.418317</td>\n      <td>2.054777</td>\n      <td>-0.128411</td>\n      <td>0.441954</td>\n      <td>...</td>\n      <td>0.950575</td>\n      <td>0.420350</td>\n      <td>0.642552</td>\n      <td>1.322419</td>\n      <td>0.893142</td>\n      <td>1.357487</td>\n      <td>0.910520</td>\n      <td>0.553791</td>\n      <td>0.623917</td>\n      <td>0.736064</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-1.008693</td>\n      <td>1.160782</td>\n      <td>-0.221900</td>\n      <td>0.478659</td>\n      <td>0.391829</td>\n      <td>0.344078</td>\n      <td>-1.826029</td>\n      <td>-0.152947</td>\n      <td>-0.212664</td>\n      <td>-0.319781</td>\n      <td>...</td>\n      <td>0.340569</td>\n      <td>0.845481</td>\n      <td>0.892774</td>\n      <td>0.557665</td>\n      <td>1.142608</td>\n      <td>1.274956</td>\n      <td>0.750241</td>\n      <td>1.639472</td>\n      <td>0.778551</td>\n      <td>0.876777</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.177668</td>\n      <td>-0.631989</td>\n      <td>0.354508</td>\n      <td>-0.721208</td>\n      <td>0.350087</td>\n      <td>1.169053</td>\n      <td>0.136857</td>\n      <td>0.177055</td>\n      <td>-0.758720</td>\n      <td>1.498564</td>\n      <td>...</td>\n      <td>1.154001</td>\n      <td>1.229276</td>\n      <td>1.045766</td>\n      <td>0.952320</td>\n      <td>1.189342</td>\n      <td>0.965396</td>\n      <td>1.080109</td>\n      <td>1.117570</td>\n      <td>0.922300</td>\n      <td>0.839124</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.338410</td>\n      <td>-0.190189</td>\n      <td>0.074796</td>\n      <td>-0.044014</td>\n      <td>-0.744120</td>\n      <td>-0.078219</td>\n      <td>0.389375</td>\n      <td>0.363241</td>\n      <td>0.718493</td>\n      <td>0.178702</td>\n      <td>...</td>\n      <td>0.611949</td>\n      <td>0.398364</td>\n      <td>0.841575</td>\n      <td>1.803682</td>\n      <td>0.905942</td>\n      <td>1.759225</td>\n      <td>0.750258</td>\n      <td>0.963737</td>\n      <td>1.569297</td>\n      <td>0.562372</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.026779</td>\n      <td>0.418469</td>\n      <td>0.012713</td>\n      <td>0.640000</td>\n      <td>0.035729</td>\n      <td>-0.423393</td>\n      <td>-0.401305</td>\n      <td>0.066129</td>\n      <td>0.225000</td>\n      <td>1.053589</td>\n      <td>...</td>\n      <td>0.727498</td>\n      <td>0.601820</td>\n      <td>0.907855</td>\n      <td>0.824140</td>\n      <td>0.741622</td>\n      <td>0.793998</td>\n      <td>0.793528</td>\n      <td>0.980815</td>\n      <td>0.906199</td>\n      <td>0.745458</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.711932</td>\n      <td>1.881008</td>\n      <td>-0.205136</td>\n      <td>0.068042</td>\n      <td>-0.645405</td>\n      <td>0.469290</td>\n      <td>-0.173488</td>\n      <td>1.053547</td>\n      <td>-0.403116</td>\n      <td>-0.452387</td>\n      <td>...</td>\n      <td>0.698473</td>\n      <td>0.579019</td>\n      <td>1.058151</td>\n      <td>0.763656</td>\n      <td>0.659289</td>\n      <td>0.904778</td>\n      <td>0.691719</td>\n      <td>0.811686</td>\n      <td>0.641338</td>\n      <td>0.603422</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.206989</td>\n      <td>0.368045</td>\n      <td>0.451437</td>\n      <td>-0.305220</td>\n      <td>0.104208</td>\n      <td>0.130911</td>\n      <td>-0.826910</td>\n      <td>1.397730</td>\n      <td>0.347226</td>\n      <td>-1.797023</td>\n      <td>...</td>\n      <td>0.471513</td>\n      <td>0.371195</td>\n      <td>0.529332</td>\n      <td>0.398287</td>\n      <td>0.719298</td>\n      <td>0.819254</td>\n      <td>0.126255</td>\n      <td>0.358638</td>\n      <td>0.615312</td>\n      <td>0.464150</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.790264</td>\n      <td>-0.490761</td>\n      <td>-0.296272</td>\n      <td>-1.581116</td>\n      <td>0.239142</td>\n      <td>-0.217248</td>\n      <td>-0.480448</td>\n      <td>0.383090</td>\n      <td>0.012946</td>\n      <td>-1.059767</td>\n      <td>...</td>\n      <td>0.907239</td>\n      <td>0.521250</td>\n      <td>0.568995</td>\n      <td>0.818254</td>\n      <td>0.949733</td>\n      <td>1.753522</td>\n      <td>0.916861</td>\n      <td>0.819690</td>\n      <td>1.009051</td>\n      <td>0.484069</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.014010</td>\n      <td>0.586256</td>\n      <td>-0.973766</td>\n      <td>0.215011</td>\n      <td>-0.220170</td>\n      <td>-0.132289</td>\n      <td>0.135526</td>\n      <td>1.820222</td>\n      <td>0.051257</td>\n      <td>-0.678915</td>\n      <td>...</td>\n      <td>0.647399</td>\n      <td>0.786828</td>\n      <td>0.262157</td>\n      <td>0.430094</td>\n      <td>0.456120</td>\n      <td>0.538695</td>\n      <td>0.233223</td>\n      <td>0.213230</td>\n      <td>0.598305</td>\n      <td>0.597890</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.182094</td>\n      <td>0.056378</td>\n      <td>-0.676597</td>\n      <td>0.491842</td>\n      <td>-0.121874</td>\n      <td>0.149285</td>\n      <td>0.814536</td>\n      <td>0.959054</td>\n      <td>0.091563</td>\n      <td>0.072141</td>\n      <td>...</td>\n      <td>0.557015</td>\n      <td>0.277199</td>\n      <td>0.985541</td>\n      <td>1.229716</td>\n      <td>0.919721</td>\n      <td>1.383838</td>\n      <td>1.215840</td>\n      <td>0.708408</td>\n      <td>1.272766</td>\n      <td>0.583322</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.101774</td>\n      <td>-0.528102</td>\n      <td>-0.076403</td>\n      <td>0.872357</td>\n      <td>0.179179</td>\n      <td>-0.259624</td>\n      <td>0.074282</td>\n      <td>-0.002134</td>\n      <td>-0.386564</td>\n      <td>-0.082914</td>\n      <td>...</td>\n      <td>0.994882</td>\n      <td>0.974937</td>\n      <td>1.347045</td>\n      <td>1.076800</td>\n      <td>0.932643</td>\n      <td>0.779570</td>\n      <td>0.795571</td>\n      <td>0.913330</td>\n      <td>0.622058</td>\n      <td>0.980927</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.265762</td>\n      <td>0.420327</td>\n      <td>0.454712</td>\n      <td>0.302573</td>\n      <td>1.493910</td>\n      <td>-0.314175</td>\n      <td>1.389485</td>\n      <td>1.107427</td>\n      <td>0.147905</td>\n      <td>-0.614600</td>\n      <td>...</td>\n      <td>0.465867</td>\n      <td>0.450466</td>\n      <td>0.356457</td>\n      <td>0.198023</td>\n      <td>0.461339</td>\n      <td>0.828802</td>\n      <td>0.574692</td>\n      <td>0.568767</td>\n      <td>0.587944</td>\n      <td>0.603948</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.059892</td>\n      <td>0.633167</td>\n      <td>0.761280</td>\n      <td>0.760854</td>\n      <td>-0.119647</td>\n      <td>-0.438942</td>\n      <td>0.747495</td>\n      <td>-0.197692</td>\n      <td>0.081559</td>\n      <td>-0.151836</td>\n      <td>...</td>\n      <td>0.351277</td>\n      <td>0.714060</td>\n      <td>0.634774</td>\n      <td>0.634865</td>\n      <td>0.673263</td>\n      <td>0.619485</td>\n      <td>0.554539</td>\n      <td>0.691272</td>\n      <td>0.550282</td>\n      <td>0.675595</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.626487</td>\n      <td>-0.158431</td>\n      <td>-0.676748</td>\n      <td>-0.803616</td>\n      <td>0.167069</td>\n      <td>-0.676502</td>\n      <td>0.816456</td>\n      <td>-0.367254</td>\n      <td>-0.419163</td>\n      <td>-0.968038</td>\n      <td>...</td>\n      <td>0.766764</td>\n      <td>0.627427</td>\n      <td>0.562448</td>\n      <td>0.635870</td>\n      <td>0.519491</td>\n      <td>0.530216</td>\n      <td>0.687517</td>\n      <td>0.715920</td>\n      <td>0.882023</td>\n      <td>0.839218</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.866298</td>\n      <td>-0.098429</td>\n      <td>-0.573917</td>\n      <td>0.431163</td>\n      <td>1.353579</td>\n      <td>1.072432</td>\n      <td>-0.322764</td>\n      <td>0.234283</td>\n      <td>1.271205</td>\n      <td>1.618540</td>\n      <td>...</td>\n      <td>0.789729</td>\n      <td>0.781411</td>\n      <td>0.885491</td>\n      <td>0.940210</td>\n      <td>0.734011</td>\n      <td>0.830063</td>\n      <td>0.840348</td>\n      <td>0.702657</td>\n      <td>0.920904</td>\n      <td>0.882827</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.218577</td>\n      <td>-0.386367</td>\n      <td>-0.161368</td>\n      <td>-0.327313</td>\n      <td>0.601797</td>\n      <td>-0.326254</td>\n      <td>-2.164355</td>\n      <td>0.071194</td>\n      <td>-0.438110</td>\n      <td>-0.937343</td>\n      <td>...</td>\n      <td>1.069480</td>\n      <td>1.049201</td>\n      <td>1.067519</td>\n      <td>1.421555</td>\n      <td>0.763547</td>\n      <td>0.841344</td>\n      <td>1.080927</td>\n      <td>1.096643</td>\n      <td>0.901883</td>\n      <td>0.789802</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.072564</td>\n      <td>0.199435</td>\n      <td>0.147559</td>\n      <td>-0.179394</td>\n      <td>-0.047295</td>\n      <td>-0.479883</td>\n      <td>0.235401</td>\n      <td>0.677919</td>\n      <td>0.256666</td>\n      <td>1.348302</td>\n      <td>...</td>\n      <td>0.549596</td>\n      <td>0.691904</td>\n      <td>0.959037</td>\n      <td>1.003023</td>\n      <td>0.896519</td>\n      <td>1.241046</td>\n      <td>0.791739</td>\n      <td>0.844695</td>\n      <td>1.036814</td>\n      <td>0.744169</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.187944</td>\n      <td>0.021905</td>\n      <td>0.226958</td>\n      <td>0.127067</td>\n      <td>-0.443422</td>\n      <td>-0.989702</td>\n      <td>0.835364</td>\n      <td>0.506010</td>\n      <td>0.050489</td>\n      <td>-0.110617</td>\n      <td>...</td>\n      <td>0.626207</td>\n      <td>0.593061</td>\n      <td>0.797011</td>\n      <td>0.499654</td>\n      <td>0.919041</td>\n      <td>1.026195</td>\n      <td>0.504378</td>\n      <td>0.714164</td>\n      <td>0.648095</td>\n      <td>1.035227</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.802707</td>\n      <td>0.666476</td>\n      <td>-0.064805</td>\n      <td>0.980947</td>\n      <td>-0.188047</td>\n      <td>-0.385459</td>\n      <td>-0.516367</td>\n      <td>0.324996</td>\n      <td>0.255761</td>\n      <td>-0.142576</td>\n      <td>...</td>\n      <td>0.525160</td>\n      <td>0.543367</td>\n      <td>0.769446</td>\n      <td>0.729830</td>\n      <td>0.529320</td>\n      <td>0.599139</td>\n      <td>0.571681</td>\n      <td>0.606157</td>\n      <td>0.419830</td>\n      <td>0.475757</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.286294</td>\n      <td>0.139949</td>\n      <td>-0.433095</td>\n      <td>-0.043339</td>\n      <td>1.264618</td>\n      <td>-0.467620</td>\n      <td>-0.611925</td>\n      <td>0.102487</td>\n      <td>0.166288</td>\n      <td>0.618344</td>\n      <td>...</td>\n      <td>0.449427</td>\n      <td>0.437719</td>\n      <td>0.555973</td>\n      <td>0.478075</td>\n      <td>0.380226</td>\n      <td>0.647384</td>\n      <td>0.269978</td>\n      <td>0.437252</td>\n      <td>0.531260</td>\n      <td>0.564383</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.264532</td>\n      <td>-0.343811</td>\n      <td>-0.577846</td>\n      <td>-0.340454</td>\n      <td>0.038294</td>\n      <td>0.636407</td>\n      <td>0.109409</td>\n      <td>2.787275</td>\n      <td>0.531051</td>\n      <td>0.593885</td>\n      <td>...</td>\n      <td>0.561514</td>\n      <td>0.710776</td>\n      <td>0.243444</td>\n      <td>0.458907</td>\n      <td>0.652025</td>\n      <td>1.397664</td>\n      <td>0.752199</td>\n      <td>0.594811</td>\n      <td>0.605597</td>\n      <td>1.502583</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.117901</td>\n      <td>0.176652</td>\n      <td>0.410014</td>\n      <td>0.427881</td>\n      <td>2.424198</td>\n      <td>1.109578</td>\n      <td>0.262734</td>\n      <td>-0.513797</td>\n      <td>0.417501</td>\n      <td>-2.378573</td>\n      <td>...</td>\n      <td>0.680568</td>\n      <td>0.784288</td>\n      <td>0.687458</td>\n      <td>0.635623</td>\n      <td>0.598925</td>\n      <td>0.450300</td>\n      <td>0.664421</td>\n      <td>0.602573</td>\n      <td>0.977902</td>\n      <td>0.496523</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.606390</td>\n      <td>-0.250953</td>\n      <td>-0.423623</td>\n      <td>0.862232</td>\n      <td>1.236941</td>\n      <td>-0.555238</td>\n      <td>-0.240114</td>\n      <td>2.005884</td>\n      <td>0.229817</td>\n      <td>-1.170817</td>\n      <td>...</td>\n      <td>0.832733</td>\n      <td>0.332151</td>\n      <td>0.673872</td>\n      <td>0.470825</td>\n      <td>0.405546</td>\n      <td>0.681677</td>\n      <td>0.675035</td>\n      <td>0.356435</td>\n      <td>0.609423</td>\n      <td>0.578609</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.402983</td>\n      <td>-0.098029</td>\n      <td>-0.648626</td>\n      <td>-0.847819</td>\n      <td>-0.165234</td>\n      <td>-0.340926</td>\n      <td>-0.610192</td>\n      <td>-1.022690</td>\n      <td>-0.258544</td>\n      <td>0.137870</td>\n      <td>...</td>\n      <td>0.398340</td>\n      <td>0.664014</td>\n      <td>0.490569</td>\n      <td>0.152743</td>\n      <td>0.421731</td>\n      <td>0.537940</td>\n      <td>0.318571</td>\n      <td>0.366131</td>\n      <td>0.379435</td>\n      <td>0.421296</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.129998</td>\n      <td>-1.530500</td>\n      <td>0.425615</td>\n      <td>0.356731</td>\n      <td>1.510383</td>\n      <td>0.298601</td>\n      <td>1.325150</td>\n      <td>0.451775</td>\n      <td>0.876342</td>\n      <td>0.154158</td>\n      <td>...</td>\n      <td>0.771897</td>\n      <td>0.850772</td>\n      <td>0.774313</td>\n      <td>0.779317</td>\n      <td>0.723018</td>\n      <td>1.243650</td>\n      <td>1.365365</td>\n      <td>1.260640</td>\n      <td>0.811344</td>\n      <td>0.645193</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>-0.627209</td>\n      <td>-0.328759</td>\n      <td>1.466718</td>\n      <td>0.206775</td>\n      <td>0.283206</td>\n      <td>0.259588</td>\n      <td>-0.482470</td>\n      <td>0.839048</td>\n      <td>-0.836016</td>\n      <td>0.771969</td>\n      <td>...</td>\n      <td>0.248630</td>\n      <td>0.852799</td>\n      <td>1.054467</td>\n      <td>1.073491</td>\n      <td>0.469751</td>\n      <td>0.735968</td>\n      <td>0.664532</td>\n      <td>1.104602</td>\n      <td>0.308665</td>\n      <td>1.005152</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.701957</td>\n      <td>0.473165</td>\n      <td>0.395478</td>\n      <td>0.401670</td>\n      <td>1.266172</td>\n      <td>0.462852</td>\n      <td>-0.004709</td>\n      <td>0.551558</td>\n      <td>-0.806474</td>\n      <td>0.603992</td>\n      <td>...</td>\n      <td>0.525116</td>\n      <td>0.275932</td>\n      <td>0.600906</td>\n      <td>0.384481</td>\n      <td>0.746002</td>\n      <td>0.692177</td>\n      <td>0.750442</td>\n      <td>0.510012</td>\n      <td>0.722149</td>\n      <td>0.705228</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.692927</td>\n      <td>-0.046108</td>\n      <td>-0.993510</td>\n      <td>0.325574</td>\n      <td>-0.144758</td>\n      <td>0.749779</td>\n      <td>-1.091492</td>\n      <td>-0.905298</td>\n      <td>-0.361705</td>\n      <td>0.367910</td>\n      <td>...</td>\n      <td>0.953161</td>\n      <td>0.533213</td>\n      <td>0.674875</td>\n      <td>0.743851</td>\n      <td>0.706055</td>\n      <td>0.795700</td>\n      <td>0.719050</td>\n      <td>0.744596</td>\n      <td>0.625651</td>\n      <td>0.665553</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>-0.914123</td>\n      <td>-0.900474</td>\n      <td>-0.150320</td>\n      <td>-0.404613</td>\n      <td>-0.275390</td>\n      <td>-0.715905</td>\n      <td>0.048271</td>\n      <td>-0.013360</td>\n      <td>-0.295373</td>\n      <td>-0.543669</td>\n      <td>...</td>\n      <td>0.897693</td>\n      <td>0.324590</td>\n      <td>0.519305</td>\n      <td>0.420123</td>\n      <td>0.485783</td>\n      <td>0.707940</td>\n      <td>0.412535</td>\n      <td>0.496743</td>\n      <td>0.406778</td>\n      <td>0.871189</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>-0.126119</td>\n      <td>1.328713</td>\n      <td>0.159579</td>\n      <td>0.417866</td>\n      <td>-0.222756</td>\n      <td>-1.191773</td>\n      <td>0.730492</td>\n      <td>1.378051</td>\n      <td>0.205499</td>\n      <td>1.236511</td>\n      <td>...</td>\n      <td>0.740850</td>\n      <td>0.955943</td>\n      <td>0.481114</td>\n      <td>0.824568</td>\n      <td>0.540927</td>\n      <td>1.745259</td>\n      <td>0.975004</td>\n      <td>0.710905</td>\n      <td>0.785248</td>\n      <td>0.483595</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>-0.568768</td>\n      <td>-0.244131</td>\n      <td>0.627234</td>\n      <td>-0.048669</td>\n      <td>-0.993898</td>\n      <td>0.306194</td>\n      <td>-0.479014</td>\n      <td>-0.298853</td>\n      <td>0.047103</td>\n      <td>0.978920</td>\n      <td>...</td>\n      <td>0.484060</td>\n      <td>0.488738</td>\n      <td>0.304058</td>\n      <td>0.366015</td>\n      <td>0.724859</td>\n      <td>0.519312</td>\n      <td>0.411944</td>\n      <td>0.262892</td>\n      <td>0.341080</td>\n      <td>0.436073</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.675976</td>\n      <td>0.855875</td>\n      <td>0.293531</td>\n      <td>0.825723</td>\n      <td>1.368092</td>\n      <td>-0.286602</td>\n      <td>-0.560994</td>\n      <td>0.598876</td>\n      <td>0.676910</td>\n      <td>-1.282450</td>\n      <td>...</td>\n      <td>0.849813</td>\n      <td>0.750543</td>\n      <td>0.704989</td>\n      <td>0.655117</td>\n      <td>0.790406</td>\n      <td>1.528952</td>\n      <td>1.148793</td>\n      <td>0.494938</td>\n      <td>0.828814</td>\n      <td>0.804193</td>\n    </tr>\n  </tbody>\n</table>\n<p>36 rows Ã— 5200 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "# for f in X.columns:\n",
    "    # X[f] = lbl.fit_transform(X[f].astype(str))\n",
    "\n",
    "# y = lbl.fit_transform(y.astype(str))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# define data_dmatrix\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y,\n",
    "                           # enable_categorical=True\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **4.5 Split data into separate training and test set** <a class=\"anchor\" id=\"4.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "0     14.0\n1     47.0\n2     41.5\n3     27.0\n4     17.0\n5     48.5\n6     75.0\n7     15.0\n8     32.0\n9     39.0\n10    34.0\n11    29.5\n12    73.0\n13    21.0\n14    12.0\n15    17.0\n16    67.0\n17    12.5\n18     8.5\n19    34.0\n20    34.0\n21    57.0\n22    11.5\n23    38.0\n24    25.0\n25    12.0\n26    39.0\n27    57.0\n28    45.0\n29    26.0\n30    39.0\n31    44.0\n32    21.5\n33    46.0\n34    62.0\n35    55.0\nName: Score_median_first20, dtype: float64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "31    44.0\n20    34.0\n16    67.0\n30    39.0\n22    11.5\n15    17.0\n10    34.0\n2     41.5\n11    29.5\n29    26.0\n27    57.0\nName: Score_median_first20, dtype: float64"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(alpha=10, learning_rate=1.0, max_depth=4,\n              objective='multi:softprob')",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(alpha=10, learning_rate=1.0, max_depth=4,\n              objective=&#x27;multi:softprob&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(alpha=10, learning_rate=1.0, max_depth=4,\n              objective=&#x27;multi:softprob&#x27;)</pre></div></div></div></div></div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# declare parameters\n",
    "params = {\n",
    "            'objective':'binary:logistic',\n",
    "            'max_depth': 4,\n",
    "            'alpha': 10,\n",
    "            'learning_rate': 1.0,\n",
    "            'n_estimators':100\n",
    "        }         \n",
    "           \n",
    "          \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(alpha=10, learning_rate=1.0, max_depth=4,\n",
      "              objective='multi:softprob')\n"
     ]
    }
   ],
   "source": [
    "# we can view the parameters of the xgb trained model as follows -\n",
    "\n",
    "print(xgb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "31    44.0\n20    34.0\n16    67.0\n30    39.0\n22    11.5\n15    17.0\n10    34.0\n2     41.5\n11    29.5\n29    26.0\n27    57.0\nName: Score_median_first20, dtype: float64"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "array([15., 15., 15., 39., 15., 12., 15., 15., 39., 39., 15.])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model mean sq error: 639.0682\n"
     ]
    }
   ],
   "source": [
    "# compute and print accuracy score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('XGBoost model mean sq error: {0:0.4f}'. format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that XGBoost obtain very high accuracy score of 91.67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **5. k-fold Cross Validation using XGBoost** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- To build more robust models with XGBoost, we should always perform k-fold cross validation. \n",
    "\n",
    "- In this way, we ensure that the original training dataset is used for both training and validation. \n",
    "\n",
    "- Also, each entry is used for validation just once. \n",
    "\n",
    "- XGBoost supports k-fold cross validation using the cv() method. \n",
    "\n",
    "- In this method, we will specify several parameters which are as follows:-\n",
    "\n",
    "  - **nfolds** - This parameter specifies the number of cross-validation sets we want to build.\n",
    "\n",
    "  - **num_boost_round** - It denotes the number of trees we build.\n",
    "\n",
    "  - **metrics** - It is the performance evaluation metrics to be considered during CV.\n",
    "\n",
    "  - **as_pandas** - It is used to return the results in a pandas DataFrame.\n",
    "\n",
    "  - **early_stopping_rounds** - This parameter stops training of the model early if the hold-out metric does not improve for a given number of rounds.\n",
    "\n",
    "  - **seed** - This parameter is used for reproducibility of results.\n",
    "\n",
    "We can use these parameters to build a k-fold cross-validation model by calling XGBoost's CV() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[14:03:41] src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000016da1cc08 dmlc::LogMessageFatal::~LogMessageFatal() + 64\n  [bt] (1) 2   libxgboost.dylib                    0x000000016da8f770 xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*) + 920\n  [bt] (2) 3   libxgboost.dylib                    0x000000016da19c1c xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*) + 1004\n  [bt] (3) 4   libxgboost.dylib                    0x000000016da32e18 XGBoosterUpdateOneIter + 212\n  [bt] (4) 5   libffi.dylib                        0x00000001a7b7c050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001a7b84ae8 ffi_call_int + 1208\n  [bt] (6) 7   _ctypes.cpython-38-darwin.so        0x000000010186a7bc _ctypes_callproc + 1440\n  [bt] (7) 8   _ctypes.cpython-38-darwin.so        0x00000001018635e8 PyCFuncPtr_call + 1172\n  [bt] (8) 9   Python3                             0x00000001013da3a8 _PyObject_MakeTpCall + 372\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mXGBoostError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mxgboost\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cv\n\u001B[1;32m      3\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary:logistic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolsample_bytree\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.3\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m      4\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m5\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malpha\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m10\u001B[39m}\n\u001B[0;32m----> 6\u001B[0m xgb_cv \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dmatrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnfold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mnum_boost_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_pandas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m123\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/datasci/lib/python3.8/site-packages/xgboost/training.py:445\u001B[0m, in \u001B[0;36mcv\u001B[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001B[0m\n\u001B[1;32m    437\u001B[0m     cb(CallbackEnv(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    438\u001B[0m                    cvfolds\u001B[38;5;241m=\u001B[39mcvfolds,\n\u001B[1;32m    439\u001B[0m                    iteration\u001B[38;5;241m=\u001B[39mi,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    442\u001B[0m                    rank\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    443\u001B[0m                    evaluation_result_list\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold \u001B[38;5;129;01min\u001B[39;00m cvfolds:\n\u001B[0;32m--> 445\u001B[0m     \u001B[43mfold\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    446\u001B[0m res \u001B[38;5;241m=\u001B[39m aggcv([f\u001B[38;5;241m.\u001B[39meval(i, feval) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m cvfolds])\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, mean, std \u001B[38;5;129;01min\u001B[39;00m res:\n",
      "File \u001B[0;32m~/PycharmProjects/datasci/lib/python3.8/site-packages/xgboost/training.py:230\u001B[0m, in \u001B[0;36mCVPack.update\u001B[0;34m(self, iteration, fobj)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, iteration, fobj):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;124;03m\"\"\"\"Update the boosters for one iteration\"\"\"\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/datasci/lib/python3.8/site-packages/xgboost/core.py:1108\u001B[0m, in \u001B[0;36mBooster.update\u001B[0;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[1;32m   1105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_features(dtrain)\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1108\u001B[0m     \u001B[43m_check_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1109\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1111\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain)\n",
      "File \u001B[0;32m~/PycharmProjects/datasci/lib/python3.8/site-packages/xgboost/core.py:176\u001B[0m, in \u001B[0;36m_check_call\u001B[0;34m(ret)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;124;03m\"\"\"Check the return value of C API call\u001B[39;00m\n\u001B[1;32m    166\u001B[0m \n\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03mThis function will raise exception when error occurs.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    return value from API calls\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 176\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m XGBoostError(py_str(_LIB\u001B[38;5;241m.\u001B[39mXGBGetLastError()))\n",
      "\u001B[0;31mXGBoostError\u001B[0m: [14:03:41] src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000016da1cc08 dmlc::LogMessageFatal::~LogMessageFatal() + 64\n  [bt] (1) 2   libxgboost.dylib                    0x000000016da8f770 xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*) + 920\n  [bt] (2) 3   libxgboost.dylib                    0x000000016da19c1c xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*) + 1004\n  [bt] (3) 4   libxgboost.dylib                    0x000000016da32e18 XGBoosterUpdateOneIter + 212\n  [bt] (4) 5   libffi.dylib                        0x00000001a7b7c050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001a7b84ae8 ffi_call_int + 1208\n  [bt] (6) 7   _ctypes.cpython-38-darwin.so        0x000000010186a7bc _ctypes_callproc + 1440\n  [bt] (7) 8   _ctypes.cpython-38-darwin.so        0x00000001018635e8 PyCFuncPtr_call + 1172\n  [bt] (8) 9   Python3                             0x00000001013da3a8 _PyObject_MakeTpCall + 372\n\n"
     ]
    }
   ],
   "source": [
    "from xgboost import cv\n",
    "\n",
    "params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **xgb_cv** contains train and test auc metrics for each boosting round. \n",
    "\n",
    "- Let's preview **xgb_cv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mxgb_cv\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'xgb_cv' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **6. Feature importance with XGBoost** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- XGBoost provides a way to examine the importance of each feature in the original dataset within the model. \n",
    "\n",
    "- It involves counting the number of times each feature is split on across all boosting trees in the model. \n",
    "\n",
    "- Then we visualize the result as a bar graph, with the features ordered according to how many times they appear.\n",
    "\n",
    "- XGBoost has a **plot_importance()** function that helps us to achieve this task. \n",
    "\n",
    "- Then we can visualize the features that has been given the highest important score among all the features. \n",
    "\n",
    "- Thus XGBoost provides us a way to do feature selection.\n",
    "\n",
    "- We will proceed as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_clf)\n",
    "plt.figure(figsize = (16, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- We can see that the feature `Delicassesn` has been given the highest importance score among all the features. \n",
    "\n",
    "- Based upon this importance score, we can select the features with highest importance score and discard the redundant ones.\n",
    "\n",
    "- Thus XGBoost also gives us a way to do feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **7. Results and Conclusion** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- In this kernel, we implement XGBoost with Python and Scikit-Learn to classify the customers from two different channels as Horeca (Hotel/Retail/CafÃ©) customers or Retail channel (nominal) customers.\n",
    "\n",
    "- The y labels contain values as 1 and 2. We have converted them into 0 and 1 for further analysis.\n",
    "\n",
    "- We have trained the XGBoost classifier and found the accuracy score to be 91.67%.\n",
    "\n",
    "- We have performed k-fold cross-validation with XGBoost.\n",
    "\n",
    "- We have find the most important feature in XGBoost. We did it using the plot_importance() function in XGBoost that helps us to achieve this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **8. References** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "The ideas and concepts in this kernel are taken from the following websites -\n",
    "\n",
    "-\thttps://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "\n",
    "-\thttps://blog.cambridgespark.com/getting-started-with-xgboost-3ba1488bb7d4\n",
    "\n",
    "-\thttps://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7\n",
    "\n",
    "-\thttps://heartbeat.fritz.ai/boosting-your-machine-learning-models-using-xgboost-d2cabb3e948f\n",
    "\n",
    "-\thttps://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\n",
    "\n",
    "-   https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5\n",
    "\n",
    "-   https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}